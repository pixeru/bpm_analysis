{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import wavfile\n",
        "from scipy.signal import butter, filtfilt, find_peaks\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import datetime\n",
        "import logging\n",
        "import sys\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from enum import Enum\n",
        "import json\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "\n",
        "DEFAULT_PARAMS = {\n",
        "    # =================================================================================\n",
        "    # 1. General & Preprocessing Settings\n",
        "    # Controls the initial loading and filtering of the audio.\n",
        "    # =================================================================================\n",
        "    \"downsample_factor\": 300,     # Factor to reduce sample rate. Higher = faster processing, less detail.\n",
        "    \"bandpass_freqs\": (20, 150),  # (low_hz, high_hz) for the bandpass filter.\n",
        "    \"save_filtered_wav\": False,    # If True, saves a .wav file of the filtered audio for debugging.\n",
        "\n",
        "    # =================================================================================\n",
        "    # 2. Signal Feature Detection\n",
        "    # Governs the initial identification of peaks and troughs in the audio envelope.\n",
        "    # =================================================================================\n",
        "    \"min_peak_distance_sec\": 0.05,      # Minimum time allowed between any two raw peaks.\n",
        "    \"peak_prominence_quantile\": 0.1,    # How much a spike must stand out to be considered a 'peak'.\n",
        "    \"trough_prominence_quantile\": 0.1,  # How much a dip must stand out to be considered a 'trough'.\n",
        "\n",
        "    # =================================================================================\n",
        "    # 3. Noise Estimation & Rejection\n",
        "    # Rules for calculating the dynamic noise floor and vetoing noisy peaks.\n",
        "    # =================================================================================\n",
        "    # --- 3.1. Dynamic Noise Floor ---\n",
        "    \"noise_floor_quantile\": 0.20,       # Quantile of troughs used to calculate the noise floor. (0.2 = 20th percentile).\n",
        "    \"noise_window_sec\": 10,             # Rolling window (in seconds) for calculating the dynamic noise floor.\n",
        "    \"trough_rejection_multiplier\": 4.0, # A trough N-times higher than the draft noise floor is rejected.\n",
        "\n",
        "    # --- 3.2. Peak Noise Vetoing ---\n",
        "    \"noise_confidence_threshold\": 0.6,  # A peak is rejected if its calculated \"noise confidence\" exceeds this.\n",
        "    \"trough_veto_multiplier\": 2.1,      # Vetoes a small peak if the next peak is N-times larger.\n",
        "    \"trough_noise_multiplier\": 3.0,     # Marks a peak as noisy if its preceding trough is N-times the noise floor.\n",
        "    \"strong_peak_override_ratio\": 6.0,  # A peak N-times the noise floor will bypass noise-rejection rules.\n",
        "\n",
        "    # =================================================================================\n",
        "    # 4. S1/S2 Pairing & Confidence Engine\n",
        "    # The core logic for identifying S1-S2 pairs based on timing and physiology.\n",
        "    # =================================================================================\n",
        "    # --- 4.1. Core Pairing Rules ---\n",
        "    \"pairing_confidence_threshold\": 0.50, # Confidence score required to classify two peaks as an S1-S2 pair.\n",
        "    \"s1_s2_interval_cap_sec\": 0.4,      # The absolute maximum time (seconds) allowed between S1 and S2.\n",
        "    \"s1_s2_interval_rr_fraction\": 0.7,  # The S1-S2 interval cannot be longer than this fraction of the R-R interval.\n",
        "\n",
        "    # --- 4.2. Amplitude-Based Confidence Model ---\n",
        "    \"deviation_smoothing_factor\": 0.05, # Smoothing applied to the peak-to-peak amplitude deviation series.\n",
        "    \"confidence_deviation_points\": [0.0, 0.25, 0.40, 0.80, 1.0], # X-axis for the confidence curves (normalized deviation).\n",
        "    \"confidence_curve_low_bpm\": [0.9, 0.9, 0.7, 0.1, 0.1],      # Y-axis curve for LOW heart rates (rewards similar amplitude).\n",
        "    \"confidence_curve_high_bpm\": [0.1, 0.5, 0.75, 0.65, 0],      # Y-axis curve for HIGH heart rates (rewards S1 > S2).\n",
        "\n",
        "    # --- 4.3. Physiology-Based Confidence Adjustment ---\n",
        "    \"stability_history_window\": 20,         # Number of recent beats used to determine rhythm stability.\n",
        "    \"stability_confidence_floor\": 0.60,     # At 0% pairing success, confidence is multiplied by this (e.g., a 50% reduction).\n",
        "    \"stability_confidence_ceiling\": 1.25,   # At 100% pairing success, confidence is multiplied by this (e.g., a 10% boost).\n",
        "    \"s1_s2_boost_ratio\": 1.2,               # S1 strength must be > (S2 strength * this value) to get a confidence boost.\n",
        "    \"boost_amount_min\": 0.10,               # Additive confidence boost for a \"good\" pair in an unstable section.\n",
        "    \"boost_amount_max\": 0.35,               # Additive confidence boost for a \"good\" pair in a stable section.\n",
        "    \"penalty_amount_min\": 0.10,             # Subtractive confidence penalty for a \"bad\" pair in a stable section.\n",
        "    \"penalty_amount_max\": 0.30,             # Subtractive confidence penalty for a \"bad\" pair in an unstable section.\n",
        "    \"s2_s1_ratio_low_bpm\": 1.5,             # At low BPM, allows S2 to be up to 1.5x S1 strength before penalty.\n",
        "    \"s2_s1_ratio_high_bpm\": 1.1,            # At high BPM, expects S2 to be no more than 1.1x S1 strength.\n",
        "    \"contractility_bpm_low\": 120.0,         # Below this BPM, the 'low BPM' confidence model is used.\n",
        "    \"contractility_bpm_high\": 140.0,        # Above this BPM, the 'high BPM' confidence model is used.\n",
        "    \"recovery_phase_duration_sec\": 120,     # Duration (seconds) of the high-contractility state after peak BPM.\n",
        "\n",
        "    # --- 4.4. Interval-Based Confidence Penalty ---\n",
        "    \"interval_penalty_start_factor\": 1.0,     # Penalty begins when interval > (max_interval * this value).\n",
        "    \"interval_penalty_full_factor\": 1.4,      # Penalty is at max when interval > (max_interval * this value).\n",
        "    \"interval_max_penalty\": 0.75,             # Max confidence points to subtract for a long interval.\n",
        "\n",
        "    # --- 4.5. Kick-Start Mechanism to Recover from Pairing Failure ---\n",
        "    \"kickstart_check_threshold\": 0.3,           # Only run the check if pairing_ratio is BELOW this value.\n",
        "    \"kickstart_history_beats\": 4,               # How many of the most recent beats to check.\n",
        "    \"kickstart_min_s1_candidates\": 3,           # At least this many of the recent beats must be \"Lone S1s\" to be considered.\n",
        "    \"kickstart_min_matches\": 3,                 # How many must match the \"S1 -> Noise\" pattern to trigger.\n",
        "    \"kickstart_override_ratio\": 0.60,           # The temporary pairing ratio to use if kick-start is triggered.\n",
        "\n",
        "    # =================================================================================\n",
        "    # 5. Rhythm Plausibility & Validation\n",
        "    # Rules for the algorithm's long-term BPM belief and beat-to-beat timing checks.\n",
        "    # =================================================================================\n",
        "    # --- 5.1. Long-Term BPM Belief ---\n",
        "    \"long_term_bpm_learning_rate\": 0.05,    # How quickly the BPM belief adapts to new beats.\n",
        "    \"max_bpm_change_per_beat\": 3.0,         # \"Speed limit\" on how much the BPM belief can change per beat.\n",
        "    \"min_bpm\": 40,                          # Absolute minimum BPM the algorithm will consider valid.\n",
        "    \"max_bpm\": 240,                         # Absolute maximum BPM the algorithm will consider valid.\n",
        "\n",
        "    # --- 5.2. Beat-to-Beat Validation ---\n",
        "    \"rr_interval_max_decrease_pct\": 0.45, # A new R-R interval can't be more than 45% shorter than the previous one.\n",
        "    \"rr_interval_max_increase_pct\": 0.70, # A new R-R interval can't be more than 70% longer than the previous one.\n",
        "    \"lone_s1_min_strength_ratio\": 0.30,   # A Lone S1 candidate's strength must be at least this fraction of the previous S1's.\n",
        "    \"lone_s1_forward_check_pct\": 0.50,    # A Lone S1 is rejected if the next peak is too close, implying a BPM spike.\n",
        "\n",
        "    # --- 5.4. Lone S1 Gradient Confidence Engine ---\n",
        "    \"lone_s1_confidence_threshold\": 0.50, # Final combined score needed to be accepted as a Lone S1.\n",
        "    \"lone_s1_rhythm_weight\": 0.65,         # The weight given to the rhythmic timing score (0.0 to 1.0).\n",
        "    \"lone_s1_amplitude_weight\": 0.35,      # The weight given to the amplitude consistency score.\n",
        "    \"lone_s1_rhythm_deviation_points\": [0.0, 0.15, 0.30, 0.50], # X-axis: % deviation from expected RR interval.\n",
        "    \"lone_s1_rhythm_confidence_curve\": [1.0, 0.8, 0.4, 0.0],   # Y-axis: Confidence score for rhythmic fit.\n",
        "    \"lone_s1_amplitude_ratio_points\": [0.0, 0.4, 0.7, 1.0],   # X-axis: Strength ratio compared to previous S1.\n",
        "    \"lone_s1_amplitude_confidence_curve\": [0.0, 0.4, 0.8, 1.0], # Y-axis: Confidence score for amplitude consistency.\n",
        "\n",
        "    # =================================================================================\n",
        "    # 6. Post-Processing Correction Pass\n",
        "    # Final analysis pass to identify and fix rhythmic discontinuities.\n",
        "    # =================================================================================\n",
        "    \"enable_correction_pass\": False,\n",
        "    \"rr_correction_threshold_pct\": 0.40,      # An R-R interval shorter than (Median R-R * this_value) is a \"discontinuity\".\n",
        "    \"rr_correction_long_interval_pct\": 1.70,  # An R-R interval longer than (Median R-R * this_value) is a \"gap\".\n",
        "    \"penalty_waiver_strength_ratio\": 4.0,     # Required signal-to-noise ratio for an S1 to be used in a correction.\n",
        "    \"penalty_waiver_max_s2_s1_ratio\": 2.5,    # Safety rail: S2/S1 amp ratio must be below this to allow a correction.\n",
        "    \"correction_log_level\": \"DEBUG\",          # Verbosity of the correction pass logs. Set to \"INFO\" or \"DEBUG\".\n",
        "\n",
        "    # =================================================================================\n",
        "    # 7. Output, HRV & Reporting\n",
        "    # Controls for final calculations, reports, and plots.\n",
        "    # =================================================================================\n",
        "    \"output_smoothing_window_sec\": 5,        # Time window (seconds) for smoothing the final BPM curve for display.\n",
        "    \"hrv_window_size_beats\": 40,             # Sliding window size (in beats) for HRV calculation.\n",
        "    \"hrv_step_size_beats\": 5,                # How many beats the HRV window moves in each step.\n",
        "    \"plot_amplitude_scale_factor\": 250.0,    # Adjusts the default y-axis range of the signal amplitude plot.\n",
        "    \"plot_downsample_audio_envelope\": True,  # If True, downsamples audio line traces for faster plotting.\n",
        "    \"plot_downsample_factor\": 5,             # The factor for downsampling plot traces (e.g., 5 = keep 1 of every 5 points).\n",
        "}\n",
        "\n",
        "# --- Enums and Global Helpers ---\n",
        "class PeakType(Enum):\n",
        "    \"\"\"Enumeration for classifying heartbeat peaks.\"\"\"\n",
        "    S1_PAIRED = \"S1 (Paired)\"\n",
        "    S2_PAIRED = \"S2 (Paired)\"\n",
        "    LONE_S1_VALIDATED = \"Lone S1 (Validated)\"\n",
        "    LONE_S1_CASCADE = \"Lone S1 (Corrected by Cascade Reset)\"\n",
        "    LONE_S1_LAST = \"Lone S1 (Last Peak)\"\n",
        "    NOISE = \"Noise/Rejected\"\n",
        "    S1_CORRECTED_GAP = \"S1 (Paired - Corrected from Gap)\"\n",
        "    S2_CORRECTED_GAP = \"S2 (Paired - Corrected from Gap)\"\n",
        "    S2_CORRECTED_CONFLICT = \"S2 (Paired - Corrected from Conflict)\"\n",
        "\n",
        "    @classmethod\n",
        "    def is_s1(cls, peak_type_str: str) -> bool:\n",
        "        \"\"\"Check if a string corresponds to any S1 type.\"\"\"\n",
        "        return peak_type_str.strip().startswith(\"S1\") or peak_type_str.strip().startswith(\"Lone S1\")\n",
        "\n",
        "    @classmethod\n",
        "    def is_s2(cls, peak_type_str: str) -> bool:\n",
        "        \"\"\"Check if a string corresponds to any S2 type.\"\"\"\n",
        "        return peak_type_str.strip().startswith(\"S2\")\n",
        "\n",
        "def _parse_reason_string(reason: str) -> Tuple[str, str]:\n",
        "    \"\"\"A helper to decouple reason string parsing, used by Plotter and ReportGenerator.\"\"\"\n",
        "    if not reason:\n",
        "        return \"Unknown Peak\", \"\"\n",
        "    separators = ['. Pairing Justification: ', '. Rejection: ', '. Original: ', '. ']\n",
        "    for sep in separators:\n",
        "        if sep in reason:\n",
        "            parts = reason.split(sep, 1)\n",
        "            peak_type = parts[0].strip()\n",
        "            details = parts[1].strip('[]')\n",
        "            return peak_type, details\n",
        "    return reason.strip(), \"\"\n",
        "\n",
        "# --- Setup Professional Logging ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - [%(levelname)s] - %(message)s',\n",
        "    stream=sys.stdout\n",
        ")\n",
        "\n",
        "# --- Audio Conversion (requires pydub/ffmpeg) ---\n",
        "try:\n",
        "    from pydub import AudioSegment\n",
        "except ImportError:\n",
        "    logging.warning(\"Pydub library not found. Install with 'pip install pydub'.\")\n",
        "    AudioSegment = None\n",
        "\n",
        "# --- Core Classes for Analysis Pipeline ---\n",
        "\n",
        "class PeakClassifier:\n",
        "    \"\"\"\n",
        "    Encapsulates the logic for classifying raw audio peaks into S1, S2, and Noise.\n",
        "    This class manages the state of the analysis loop, including BPM belief,\n",
        "    beat candidates, and debug information.\n",
        "    \"\"\"\n",
        "    def __init__(self, audio_envelope: np.ndarray, sample_rate: int, params: Dict,\n",
        "                 start_bpm_hint: Optional[float], precomputed_noise_floor: pd.Series,\n",
        "                 precomputed_troughs: np.ndarray, peak_bpm_time_sec: Optional[float],\n",
        "                 recovery_end_time_sec: Optional[float]):\n",
        "\n",
        "        self.audio_envelope = audio_envelope\n",
        "        self.sample_rate = sample_rate\n",
        "        self.params = params\n",
        "        self.peak_bpm_time_sec = peak_bpm_time_sec\n",
        "        self.recovery_end_time_sec = recovery_end_time_sec\n",
        "\n",
        "        self.state = self._initialize_state(\n",
        "            start_bpm_hint, precomputed_noise_floor, precomputed_troughs\n",
        "        )\n",
        "\n",
        "    def _initialize_state(self, start_bpm_hint, precomputed_noise_floor, precomputed_troughs) -> Dict:\n",
        "        \"\"\"Pre-calculates all necessary data and initializes the state for the peak finding loop.\"\"\"\n",
        "        state = {'analysis_data': {}}\n",
        "        state['dynamic_noise_floor'], state['trough_indices'] = precomputed_noise_floor, precomputed_troughs\n",
        "        state['all_peaks'] = self._find_raw_peaks(state['dynamic_noise_floor'].values)\n",
        "        state['analysis_data']['dynamic_noise_floor_series'] = state['dynamic_noise_floor']\n",
        "        state['analysis_data']['trough_indices'] = state['trough_indices']\n",
        "\n",
        "        noise_floor_at_peaks = state['dynamic_noise_floor'].reindex(state['all_peaks'], method='nearest').values\n",
        "        peak_strengths = self.audio_envelope[state['all_peaks']] - noise_floor_at_peaks\n",
        "        peak_strengths[peak_strengths < 0] = 0\n",
        "        normalized_deviations = np.abs(np.diff(peak_strengths)) / (np.maximum(peak_strengths[:-1], peak_strengths[1:]) + 1e-9)\n",
        "        deviation_times = (state['all_peaks'][:-1] + state['all_peaks'][1:]) / 2 / self.sample_rate\n",
        "        deviation_series = pd.Series(normalized_deviations, index=deviation_times)\n",
        "        smoothing_window = max(5, int(len(deviation_series) * self.params['deviation_smoothing_factor']))\n",
        "        state['smoothed_dev_series'] = deviation_series.rolling(window=smoothing_window, min_periods=1, center=True).mean()\n",
        "        state['analysis_data']['deviation_series'] = state['smoothed_dev_series']\n",
        "\n",
        "        state['long_term_bpm'] = float(start_bpm_hint) if start_bpm_hint else 80.0\n",
        "        state['candidate_beats'] = []\n",
        "        state['beat_debug_info'] = {}\n",
        "        state['long_term_bpm_history'] = []\n",
        "        state['sorted_troughs'] = sorted(state['trough_indices'])\n",
        "        state['consecutive_rr_rejections'] = 0\n",
        "        state['loop_idx'] = 0\n",
        "\n",
        "        return state\n",
        "\n",
        "    def classify_peaks(self) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
        "        \"\"\"Main classification loop to iterate through all raw peaks.\"\"\"\n",
        "        if len(self.state['all_peaks']) < 2:\n",
        "            return self.state['all_peaks'], self.state['all_peaks'], {\"beat_debug_info\": {}}\n",
        "\n",
        "        while self.state['loop_idx'] < len(self.state['all_peaks']):\n",
        "            self._kickstart_check()\n",
        "            current_peak_idx = self.state['all_peaks'][self.state['loop_idx']]\n",
        "            is_last_peak = self.state['loop_idx'] >= len(self.state['all_peaks']) - 1\n",
        "\n",
        "            if is_last_peak:\n",
        "                self._handle_last_peak(current_peak_idx)\n",
        "            else:\n",
        "                self._process_peak_pair(current_peak_idx)\n",
        "\n",
        "            self._update_long_term_bpm()\n",
        "\n",
        "        return self._finalize_results()\n",
        "\n",
        "    def _kickstart_check(self):\n",
        "        \"\"\"Specialized recovery function to kick-start the algorithm if it gets stuck.\"\"\"\n",
        "        pairing_ratio = self._calculate_pairing_ratio()\n",
        "        if pairing_ratio >= self.params.get(\"kickstart_check_threshold\", 0.3):\n",
        "            return\n",
        "\n",
        "        history = self.params.get(\"kickstart_history_beats\", 4)\n",
        "        if len(self.state['candidate_beats']) < history:\n",
        "            return\n",
        "\n",
        "        min_s1s = self.params.get(\"kickstart_min_s1_candidates\", 3)\n",
        "        recent_lone_s1s = [idx for idx in self.state['candidate_beats'][-history:] if \"Lone S1\" in self.state['beat_debug_info'].get(idx, \"\")]\n",
        "        if len(recent_lone_s1s) < min_s1s:\n",
        "            return\n",
        "\n",
        "        min_matches = self.params.get(\"kickstart_min_matches\", 3)\n",
        "        matches = 0\n",
        "        for s1_idx in recent_lone_s1s:\n",
        "            current_raw_idx = np.searchsorted(self.state['all_peaks'], s1_idx)\n",
        "            if current_raw_idx < len(self.state['all_peaks']) - 1:\n",
        "                next_raw_peak_idx = self.state['all_peaks'][current_raw_idx + 1]\n",
        "                if \"Noise\" in self.state['beat_debug_info'].get(next_raw_peak_idx, \"\"):\n",
        "                    matches += 1\n",
        "\n",
        "        if matches >= min_matches:\n",
        "            override_ratio = self.params.get(\"kickstart_override_ratio\", 0.6)\n",
        "            logging.info(f\"KICK-START: Found {matches}/{len(recent_lone_s1s)} S1->Noise patterns. Overriding pairing ratio to {override_ratio}.\")\n",
        "            # This is a temporary state change, so we don't store the override ratio in self.state\n",
        "            self.state['pairing_ratio_override'] = override_ratio\n",
        "\n",
        "    def _handle_last_peak(self, peak_idx: int):\n",
        "        \"\"\"Classify the final peak in the sequence.\"\"\"\n",
        "        self.state['candidate_beats'].append(peak_idx)\n",
        "        self.state['beat_debug_info'][peak_idx] = PeakType.LONE_S1_LAST.value\n",
        "        self.state['loop_idx'] += 1\n",
        "\n",
        "    def _process_peak_pair(self, current_peak_idx: int):\n",
        "        \"\"\"Processes a pair of peaks to determine if they are S1-S2.\"\"\"\n",
        "        next_peak_idx = self.state['all_peaks'][self.state['loop_idx'] + 1]\n",
        "        pairing_ratio = self._calculate_pairing_ratio()\n",
        "\n",
        "        is_paired, reason = self._attempt_s1_s2_pairing(\n",
        "            current_peak_idx, next_peak_idx, pairing_ratio\n",
        "        )\n",
        "\n",
        "        if is_paired:\n",
        "            self.state['candidate_beats'].append(current_peak_idx)\n",
        "            reason_tag = f\"PAIRING_SUCCESS_REASON§{reason}\"\n",
        "            self.state['beat_debug_info'][current_peak_idx] = f\"{PeakType.S1_PAIRED.value}§{reason_tag}\"\n",
        "            self.state['beat_debug_info'][next_peak_idx] = f\"{PeakType.S2_PAIRED.value}§{reason_tag}\"\n",
        "            self.state['consecutive_rr_rejections'] = 0\n",
        "            self.state['loop_idx'] += 2\n",
        "        else:\n",
        "            self._classify_lone_peak(current_peak_idx, reason)\n",
        "            self.state['loop_idx'] += 1\n",
        "\n",
        "    def _update_long_term_bpm(self):\n",
        "        \"\"\"Updates the long-term BPM belief after each decision.\"\"\"\n",
        "        if len(self.state['candidate_beats']) > 1:\n",
        "            new_rr = (self.state['candidate_beats'][-1] - self.state['candidate_beats'][-2]) / self.sample_rate\n",
        "            if new_rr > 0:\n",
        "                self.state['long_term_bpm'] = update_long_term_bpm(new_rr, self.state['long_term_bpm'], self.params)\n",
        "\n",
        "        if self.state['candidate_beats']:\n",
        "            time_sec = self.state['candidate_beats'][-1] / self.sample_rate\n",
        "            self.state['long_term_bpm_history'].append((time_sec, self.state['long_term_bpm']))\n",
        "\n",
        "    def _finalize_results(self) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
        "        \"\"\"Finalizes and returns the analysis results.\"\"\"\n",
        "        final_peaks = np.array(sorted(list(dict.fromkeys(self.state['candidate_beats']))))\n",
        "        self.state['analysis_data'][\"beat_debug_info\"] = self.state['beat_debug_info']\n",
        "        if self.state['long_term_bpm_history']:\n",
        "            lt_bpm_times, lt_bpm_values = zip(*self.state['long_term_bpm_history'])\n",
        "            self.state['analysis_data'][\"long_term_bpm_series\"] = pd.Series(lt_bpm_values, index=lt_bpm_times)\n",
        "        return final_peaks, self.state['all_peaks'], self.state['analysis_data']\n",
        "\n",
        "    def _find_raw_peaks(self, height_threshold: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Finds all potential peaks above the given height threshold.\"\"\"\n",
        "        prominence_thresh = np.quantile(self.audio_envelope, self.params['peak_prominence_quantile'])\n",
        "        min_peak_dist_samples = int(self.params['min_peak_distance_sec'] * self.sample_rate)\n",
        "        peaks, _ = find_peaks(self.audio_envelope, height=height_threshold, prominence=prominence_thresh, distance=min_peak_dist_samples)\n",
        "        logging.info(f\"Found {len(peaks)} raw peaks using dynamic height threshold.\")\n",
        "        logging.info(f\"Raw peak detection: min_peak_distance_sec={self.params['min_peak_distance_sec']}s -> {min_peak_dist_samples} samples\")\n",
        "        return peaks\n",
        "\n",
        "    def _attempt_s1_s2_pairing(self, s1_candidate_idx: int, s2_candidate_idx: int, pairing_ratio: float) -> Tuple[bool, str]:\n",
        "        \"\"\"Calculates the confidence score for pairing two candidate peaks.\"\"\"\n",
        "        interval_sec = (s2_candidate_idx - s1_candidate_idx) / self.sample_rate\n",
        "        min_peak_distance_sec = self.params['min_peak_distance_sec']\n",
        "        min_peak_distance_calc = f\"min_peak_distance_sec={min_peak_distance_sec}s (from params)\"\n",
        "        distance_check = \"PASS\" if interval_sec >= min_peak_distance_sec else \"FAIL\"\n",
        "        distance_info = f\"Interval {interval_sec:.3f}s vs {min_peak_distance_calc} -> {distance_check}\"\n",
        "\n",
        "        deviation_value = self.state['smoothed_dev_series'].asof(s1_candidate_idx / self.sample_rate)\n",
        "\n",
        "        confidence = calculate_blended_confidence(deviation_value, self.state['long_term_bpm'], self.params)\n",
        "        blend_ratio = np.clip((self.state['long_term_bpm'] - self.params['contractility_bpm_low']) / (self.params['contractility_bpm_high'] - self.params['contractility_bpm_low']), 0, 1)\n",
        "        reason = f\"Base Conf (Blended Model {blend_ratio:.0%} High): {confidence:.2f}\"\n",
        "\n",
        "        confidence, adjust_reason = _adjust_confidence_with_stability_and_ratio(\n",
        "            confidence, s1_candidate_idx, s2_candidate_idx, self.audio_envelope, self.state['dynamic_noise_floor'],\n",
        "            self.state['long_term_bpm'], pairing_ratio, self.params, self.sample_rate,\n",
        "            self.peak_bpm_time_sec, self.recovery_end_time_sec, len(self.state['candidate_beats'])\n",
        "        )\n",
        "        reason += adjust_reason\n",
        "\n",
        "        s1_s2_max_interval = min(self.params['s1_s2_interval_cap_sec'], (60.0 / self.state['long_term_bpm']) * self.params['s1_s2_interval_rr_fraction'])\n",
        "        confidence, interval_reason = _apply_interval_penalty(confidence, interval_sec, s1_s2_max_interval, self.params)\n",
        "        reason += interval_reason\n",
        "\n",
        "        is_paired = confidence >= self.params['pairing_confidence_threshold']\n",
        "        reason += f\"\\n- Final Score: {confidence:.2f} vs Threshold {self.params['pairing_confidence_threshold']:.2f} -> {'Paired' if is_paired else 'Not Paired'}\"\n",
        "\n",
        "        # Add the distance check information to the reason\n",
        "        reason = f\"{distance_info}\\n{reason}\"\n",
        "\n",
        "        return is_paired, reason\n",
        "\n",
        "    def _classify_lone_peak(self, peak_idx: int, pairing_failure_reason: str):\n",
        "        \"\"\"Validates if an unpaired peak is a Lone S1 or Noise.\"\"\"\n",
        "        is_valid, rejection_detail = self._validate_lone_s1(peak_idx)\n",
        "        pairing_info = f\"PAIRING_FAIL_REASON§{pairing_failure_reason.lstrip(' |')}\"\n",
        "\n",
        "        if is_valid:\n",
        "            self.state['candidate_beats'].append(peak_idx)\n",
        "            # For a validated S1, the \"rejection_detail\" is just the success reason.\n",
        "            self.state['beat_debug_info'][\n",
        "                peak_idx] = f\"{PeakType.LONE_S1_VALIDATED.value}§{pairing_info}§LONE_S1_VALIDATE_REASON§{rejection_detail}\"\n",
        "            self.state['consecutive_rr_rejections'] = 0\n",
        "        else:\n",
        "            is_rhythm_rejection = \"Rhythm Fit\" in rejection_detail\n",
        "            if is_rhythm_rejection:\n",
        "                self.state['consecutive_rr_rejections'] += 1\n",
        "            else:\n",
        "                self.state['consecutive_rr_rejections'] = 0\n",
        "\n",
        "            lone_s1_rejection_info = f\"LONE_S1_REJECT_REASON§{rejection_detail}\"\n",
        "\n",
        "            if self.state['consecutive_rr_rejections'] >= self.params.get(\"cascade_reset_trigger_count\", 3):\n",
        "                logging.info(\n",
        "                    f\"CASCADE RESET: Forcing peak at {peak_idx / self.sample_rate:.2f}s as Lone S1 due to repeated rhythmic failures.\")\n",
        "                self.state['candidate_beats'].append(peak_idx)\n",
        "                self.state['beat_debug_info'][\n",
        "                    peak_idx] = f\"{PeakType.LONE_S1_CASCADE.value}§{pairing_info}§{lone_s1_rejection_info}\"\n",
        "                self.state['consecutive_rr_rejections'] = 0\n",
        "            else:\n",
        "                self.state['beat_debug_info'][peak_idx] = f\"Noise§{pairing_info}§{lone_s1_rejection_info}\"\n",
        "\n",
        "    def _validate_lone_s1(self, current_peak_idx: int) -> Tuple[bool, str]:\n",
        "        \"\"\"Performs checks to determine if a peak is a valid Lone S1.\"\"\"\n",
        "        if not self.state['candidate_beats']: return True, \"First beat\"\n",
        "\n",
        "        confidence, reason = calculate_lone_s1_confidence(\n",
        "            current_peak_idx, self.state['candidate_beats'][-1], self.state['long_term_bpm'],\n",
        "            self.audio_envelope, self.state['dynamic_noise_floor'], self.sample_rate, self.params\n",
        "        )\n",
        "        threshold = self.params.get(\"lone_s1_confidence_threshold\", 0.6)\n",
        "        if confidence < threshold:\n",
        "            return False, f\"Rejected Lone S1: Confidence {confidence:.2f} < Threshold {threshold:.2f}. ({reason})\"\n",
        "\n",
        "        current_peak_all_peaks_idx = np.searchsorted(self.state['all_peaks'], current_peak_idx)\n",
        "        if current_peak_all_peaks_idx < len(self.state['all_peaks']) - 1:\n",
        "            next_raw_peak_idx = self.state['all_peaks'][current_peak_all_peaks_idx + 1]\n",
        "            forward_interval_sec = (next_raw_peak_idx - current_peak_idx) / self.sample_rate\n",
        "            expected_rr_sec = 60.0 / self.state['long_term_bpm']\n",
        "            min_forward_interval = expected_rr_sec * self.params.get('lone_s1_forward_check_pct', 0.6)\n",
        "            if forward_interval_sec < min_forward_interval:\n",
        "                if not (self.audio_envelope[current_peak_idx] > (self.audio_envelope[next_raw_peak_idx] * 1.7)):\n",
        "                     implied_bpm = 60.0 / forward_interval_sec if forward_interval_sec > 0 else float('inf')\n",
        "                     return False, f\"Rejected Lone S1: Forward check failed (Implies {implied_bpm:.0f} BPM)\"\n",
        "        return True, \"\"\n",
        "\n",
        "    def _calculate_pairing_ratio(self) -> float:\n",
        "        \"\"\"Calculates the recent rhythm stability as a ratio.\"\"\"\n",
        "        history_window = self.params.get(\"stability_history_window\", 20)\n",
        "        if len(self.state['candidate_beats']) < history_window: return 0.5\n",
        "        recent_beats = self.state['candidate_beats'][-history_window:]\n",
        "        paired_count = sum(1 for beat_idx in recent_beats if PeakType.S1_PAIRED.value in self.state['beat_debug_info'].get(beat_idx, \"\"))\n",
        "        return paired_count / history_window\n",
        "\n",
        "def format_pairing_details_list(details_str: str) -> List[str]:\n",
        "    \"\"\"Formats S1-S2 pairing details into a list of strings.\"\"\"\n",
        "    lines = [line.strip().lstrip('- ') for line in details_str.strip().split('\\n') if line.strip()]\n",
        "    if not lines: return [\"- S1-S2 pairing decision:\", \"    - No details available.\"]\n",
        "\n",
        "    output_lines = [\"- S1-S2 pairing decision:\"]\n",
        "    confidence = 0.0\n",
        "\n",
        "    try:\n",
        "        # Check if the first line contains distance information\n",
        "        if \"Interval\" in lines[0] and \"vs min_peak_distance_sec\" in lines[0]:\n",
        "            output_lines.append(f\"    - {lines[0]}\")\n",
        "            lines = lines[1:]  # Remove the distance line from processing\n",
        "\n",
        "        match = re.search(r'([\\d\\.]+)$', lines[0])\n",
        "        if match: confidence = float(match.group(1))\n",
        "        output_lines.append(f\"    - {lines[0]}\")\n",
        "\n",
        "        for line in lines[1:]:\n",
        "            new_confidence = confidence\n",
        "            if \"Stability Pre-Adjust\" in line:\n",
        "                match = re.search(r'x([\\d\\.]+)', line); new_confidence *= float(match.group(1)) if match else 1\n",
        "                output_lines.append(f\"    - {line} -> {new_confidence:.3f}\")\n",
        "            elif \"PENALIZED by\" in line:\n",
        "                match = re.search(r'by ([\\d\\.]+)', line); new_confidence -= float(match.group(1)) if match else 0\n",
        "                output_lines.append(f\"    - {line} -> {new_confidence:.3f}\")\n",
        "            elif \"Interval PENALTY by\" in line:\n",
        "                match = re.search(r'by ([\\d\\.]+)', line); new_confidence -= float(match.group(1)) if match else 0\n",
        "                output_lines.append(f\"    - {line} -> {max(0, new_confidence):.3f}\")\n",
        "            else:\n",
        "                output_lines.append(f\"    - {line}\")\n",
        "            confidence = new_confidence\n",
        "    except (ValueError, IndexError):\n",
        "        return [\"- S1-S2 pairing decision:\", f\"    - {details_str}\"]\n",
        "    return output_lines\n",
        "\n",
        "def format_lone_s1_details_list(details_str: str) -> List[str]:\n",
        "    \"\"\"Formats Lone S1 validation details into a list of strings.\"\"\"\n",
        "    output_lines = [\"- Lone S1 decision:\"]\n",
        "    try:\n",
        "        main_match = re.search(r'^(.*?)\\s*\\((.*)\\)$', details_str)\n",
        "        if not main_match:\n",
        "            return [\"- Lone S1 decision:\", f\"\\t- {details_str}\"]\n",
        "\n",
        "        decision_summary, reasons_text = main_match.group(1).strip().rstrip('.'), main_match.group(2)\n",
        "        reason_components = reasons_text.split(', ')\n",
        "        for component in reason_components:\n",
        "            parts = component.split('=', 1)\n",
        "            if len(parts) == 2:\n",
        "                name, value_str = parts[0].strip(), parts[1].strip()\n",
        "                score_match = re.match(r'([\\d\\.]+)', value_str)\n",
        "                score = score_match.group(1) if score_match else \"N/A\"\n",
        "                output_lines.append(f\"\\t- {name}: {value_str} -> {score}\")\n",
        "            else:\n",
        "                output_lines.append(f\"\\t- {component}\")\n",
        "\n",
        "        score_match = re.search(r'(.*):\\s*Confidence\\s*([\\d\\.]+)\\s*<\\s*Threshold\\s*([\\d\\.]+)', decision_summary)\n",
        "        if score_match:\n",
        "            decision_type, confidence, threshold = score_match.group(1).strip(), score_match.group(2), score_match.group(3)\n",
        "            outcome = f\"Noise ({decision_type})\"\n",
        "            output_lines.append(f\"\\t- Final Score: Confidence {confidence} vs Threshold {threshold} -> {outcome}\")\n",
        "        else:\n",
        "            output_lines.append(f\"\\t- Final Decision: {decision_summary}\")\n",
        "    except Exception:\n",
        "        return [\"- Lone S1 decision:\", f\"\\t- {details_str}\"]\n",
        "    return output_lines\n",
        "\n",
        "class Plotter:\n",
        "    \"\"\"Handles the creation and generation of the final analysis plot.\"\"\"\n",
        "    def __init__(self, file_name: str, params: Dict, sample_rate: int, output_directory: str):\n",
        "        self.file_name = file_name\n",
        "        self.params = params\n",
        "        self.sample_rate = sample_rate\n",
        "        self.output_directory = output_directory # Add this line\n",
        "        self.fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "    def plot_and_save(self, audio_envelope: np.ndarray, all_raw_peaks: np.ndarray, analysis_data: Dict,\n",
        "                      final_metrics: Dict):\n",
        "        \"\"\"Generates and saves the main analysis plot by calling helper methods.\"\"\"\n",
        "        time_axis_dt = pd.to_datetime([datetime.datetime.fromtimestamp(0) + datetime.timedelta(seconds=t) for t in (np.arange(len(audio_envelope)) / self.sample_rate)])\n",
        "\n",
        "        self._add_line_traces(time_axis_dt, audio_envelope, analysis_data)\n",
        "        self._add_trough_markers(audio_envelope, analysis_data)\n",
        "        self._add_peak_traces(all_raw_peaks, analysis_data.get('beat_debug_info', {}), audio_envelope)\n",
        "        self._add_bpm_hrv_traces(final_metrics.get('smoothed_bpm'), analysis_data, final_metrics.get('windowed_hrv_df'))\n",
        "        self._add_slope_traces(final_metrics.get('major_inclines'), final_metrics.get('major_declines'), final_metrics.get('peak_recovery_stats'), final_metrics.get('peak_exertion_stats'))\n",
        "        self._add_annotations_and_summary(final_metrics.get('smoothed_bpm'), final_metrics.get('hrv_summary'), final_metrics.get('hrr_stats'), final_metrics.get('peak_recovery_stats'))\n",
        "\n",
        "        self._configure_layout()\n",
        "\n",
        "        base_name = os.path.basename(os.path.splitext(self.file_name)[0])\n",
        "        output_html_path = os.path.join(self.output_directory, f\"{base_name}_bpm_plot.html\")\n",
        "        plot_title = f\"Heartbeat Analysis - {os.path.basename(self.file_name)}\"\n",
        "        plot_config = {'scrollZoom': True, 'toImageButtonOptions': {'filename': plot_title, 'format': 'png', 'scale': 2}}\n",
        "        self.fig.write_html(output_html_path, config=plot_config)\n",
        "        logging.info(f\"Interactive plot saved to {output_html_path}\")\n",
        "\n",
        "    def _configure_layout(self):\n",
        "        \"\"\"Sets up the plot layout, titles, and axes.\"\"\"\n",
        "        plot_title = f\"Heartbeat Analysis - {os.path.basename(self.file_name)}\"\n",
        "        self.fig.update_layout(\n",
        "            template=\"plotly_dark\", title_text=plot_title, dragmode='pan',\n",
        "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
        "            margin=dict(t=140, b=100),\n",
        "            xaxis=dict(title_text=\"Time (mm:ss)\", tickformat='%M:%S', hoverformat='%M:%S'),\n",
        "            hovermode='x unified'\n",
        "        )\n",
        "        robust_upper_limit = np.quantile(self.fig.data[0].y, 0.95) if self.fig.data else 1\n",
        "        amplitude_scale = self.params.get(\"plot_amplitude_scale_factor\", 60.0)\n",
        "        self.fig.update_yaxes(title_text=\"Signal Amplitude\", secondary_y=False, range=[0, robust_upper_limit * amplitude_scale])\n",
        "        self.fig.update_yaxes(title_text=\"BPM / HRV\", secondary_y=True, range=[50, 200])\n",
        "\n",
        "    def _add_line_traces(self, time_axis_dt: pd.Series, audio_envelope: np.ndarray, analysis_data: Dict):\n",
        "        \"\"\"Adds downsampled audio envelope and noise floor traces for performance.\"\"\"\n",
        "        # --- Prepare data for plotting, with optional downsampling for performance ---\n",
        "        plot_time_axis_dt = time_axis_dt\n",
        "        plot_envelope = audio_envelope\n",
        "        plot_noise_floor = analysis_data.get('dynamic_noise_floor_series')\n",
        "\n",
        "        if self.params.get(\"plot_downsample_audio_envelope\", False):\n",
        "            factor = self.params.get(\"plot_downsample_factor\", 5)\n",
        "            if factor > 1 and len(audio_envelope) >= factor:\n",
        "                logging.info(f\"Downsampling line traces by a factor of {factor} for plotting.\")\n",
        "                plot_time_axis_dt = time_axis_dt[::factor]\n",
        "                plot_envelope = audio_envelope[::factor]\n",
        "                if plot_noise_floor is not None and not plot_noise_floor.empty:\n",
        "                    plot_noise_floor = plot_noise_floor.iloc[::factor]\n",
        "\n",
        "        # --- Add the potentially downsampled line traces ---\n",
        "        self.fig.add_trace(go.Scatter(\n",
        "            x=plot_time_axis_dt,\n",
        "            y=plot_envelope,\n",
        "            name=\"Audio Envelope\",\n",
        "            line=dict(color=\"#47a5c4\")),\n",
        "            secondary_y=False)\n",
        "        if plot_noise_floor is not None and not plot_noise_floor.empty and len(plot_noise_floor) >= len(plot_time_axis_dt):\n",
        "            self.fig.add_trace(go.Scatter(\n",
        "                x=plot_time_axis_dt,\n",
        "                y=plot_noise_floor.values,\n",
        "                name=\"Dynamic Noise Floor\",\n",
        "                line=dict(color=\"green\", dash=\"dot\", width=1.5),\n",
        "                hovertemplate=\"Noise Floor: %{y:.2f}<extra></extra>\"),\n",
        "                secondary_y=False)\n",
        "\n",
        "    def _add_trough_markers(self, audio_envelope: np.ndarray, analysis_data: Dict):\n",
        "        \"\"\"Adds trough markers to the plot using original full-resolution data for accuracy.\"\"\"\n",
        "        trough_indices = analysis_data.get('trough_indices')\n",
        "        if trough_indices is not None and trough_indices.size > 0:\n",
        "            # Create datetime objects for the trough markers\n",
        "            trough_times_dt = pd.to_datetime([\n",
        "                datetime.datetime.fromtimestamp(0) + datetime.timedelta(seconds=t)\n",
        "                for t in (trough_indices / self.sample_rate)\n",
        "            ])\n",
        "\n",
        "            self.fig.add_trace(go.Scatter(\n",
        "                x=trough_times_dt,\n",
        "                y=audio_envelope[trough_indices],\n",
        "                mode='markers',\n",
        "                name='Troughs',\n",
        "                marker=dict(color='green', symbol='circle-open', size=6),\n",
        "                visible='legendonly'),\n",
        "                secondary_y=False)\n",
        "\n",
        "    def _add_peak_traces(self, all_raw_peaks, debug_info, audio_envelope):\n",
        "        \"\"\"Adds S1, S2, and Noise peak markers to the plot with detailed hover info.\"\"\"\n",
        "        s1_peaks = {'indices': [], 'customdata': []}\n",
        "        s2_peaks = {'indices': [], 'customdata': []}\n",
        "        noise_peaks = {'indices': [], 'customdata': []}\n",
        "\n",
        "        classified_indices = set()\n",
        "\n",
        "        # --- Generate detailed hover text for each classified peak ---\n",
        "        for peak_idx, reason_str in debug_info.items():\n",
        "            hover_text_parts = []\n",
        "            parts = reason_str.split('§')\n",
        "            final_peak_type, details_list = parts[0], parts[1:]\n",
        "\n",
        "            # Add basic peak info\n",
        "            hover_text_parts.append(f\"<b>Type:</b> {final_peak_type}\")\n",
        "            hover_text_parts.append(f\"<b>Time:</b> {peak_idx / self.sample_rate:.2f}s\")\n",
        "            hover_text_parts.append(f\"<b>Amp:</b> {audio_envelope[peak_idx]:.0f}\")\n",
        "            hover_text_parts.append(\"---\")  # Visual separator\n",
        "\n",
        "            # Add detailed, formatted reasons from the debug string\n",
        "            i = 0\n",
        "            while i < len(details_list):\n",
        "                tag = details_list[i]\n",
        "                value = details_list[i + 1] if (i + 1) < len(details_list) else \"\"\n",
        "                formatted_lines = []\n",
        "\n",
        "                if \"PAIRING\" in tag:\n",
        "                    formatted_lines = format_pairing_details_list(value)\n",
        "                elif \"LONE_S1_REJECT_REASON\" in tag:\n",
        "                    formatted_lines = format_lone_s1_details_list(value)\n",
        "                elif \"LONE_S1_VALIDATE_REASON\" in tag:\n",
        "                    formatted_lines = [\"- Lone S1 decision:\", f\"&nbsp;&nbsp;&nbsp;&nbsp;- Validated: {value}\"]\n",
        "                elif \"ORIGINAL_REASON\" in tag:\n",
        "                    formatted_lines = [\"- Original Classification:\",\n",
        "                                       f\"&nbsp;&nbsp;&nbsp;&nbsp;- {value.replace('`', '')}\"]\n",
        "\n",
        "                if formatted_lines:\n",
        "                    # Convert the list of strings to a single HTML block\n",
        "                    sub_text = \"<br>\".join(l.replace('\\t', '&nbsp;&nbsp;&nbsp;&nbsp;') for l in formatted_lines)\n",
        "                    hover_text_parts.append(sub_text)\n",
        "                i += 2\n",
        "\n",
        "            # Join all parts into a single HTML string for the tooltip\n",
        "            full_hover_text = \"<br>\".join(hover_text_parts)\n",
        "            classified_indices.add(peak_idx)\n",
        "            peak_type, _ = _parse_reason_string(reason_str)\n",
        "\n",
        "            # Assign the peak to the correct category for plotting\n",
        "            if PeakType.is_s1(peak_type):\n",
        "                s1_peaks['indices'].append(peak_idx)\n",
        "                s1_peaks['customdata'].append(full_hover_text)\n",
        "            elif PeakType.is_s2(peak_type):\n",
        "                s2_peaks['indices'].append(peak_idx)\n",
        "                s2_peaks['customdata'].append(full_hover_text)\n",
        "            else:\n",
        "                noise_peaks['indices'].append(peak_idx)\n",
        "                noise_peaks['customdata'].append(full_hover_text)\n",
        "\n",
        "        # --- Handle any raw peaks that were never classified ---\n",
        "        for peak_idx in all_raw_peaks:\n",
        "            if peak_idx not in classified_indices:\n",
        "                hover_text = (f\"<b>Type:</b> Unclassified<br>\"\n",
        "                              f\"<b>Time:</b> {peak_idx / self.sample_rate:.2f}s<br>\"\n",
        "                              f\"<b>Amp:</b> {audio_envelope[peak_idx]:.0f}<br>\"\n",
        "                              \"<b>Details:</b> Peak was not evaluated by the classifier.\")\n",
        "                noise_peaks['indices'].append(peak_idx)\n",
        "                noise_peaks['customdata'].append(hover_text)\n",
        "\n",
        "        # A simplified hovertemplate that displays the pre-formatted custom data\n",
        "        hovertemplate = \"%{customdata}<extra></extra>\"\n",
        "\n",
        "        # --- Add traces to the plot ---\n",
        "        if s1_peaks['indices']:\n",
        "            times_dt = pd.to_datetime([datetime.datetime.fromtimestamp(0) + datetime.timedelta(seconds=t) for t in\n",
        "                                       (np.array(s1_peaks['indices']) / self.sample_rate)])\n",
        "            self.fig.add_trace(\n",
        "                go.Scatter(x=times_dt, y=audio_envelope[s1_peaks['indices']], mode='markers', name='S1 Beats',\n",
        "                           marker=dict(color='#e36f6f', size=8, symbol='diamond'),\n",
        "                           customdata=s1_peaks['customdata'],\n",
        "                           hovertemplate=hovertemplate), secondary_y=False)\n",
        "\n",
        "        if s2_peaks['indices']:\n",
        "            times_dt = pd.to_datetime([datetime.datetime.fromtimestamp(0) + datetime.timedelta(seconds=t) for t in\n",
        "                                       (np.array(s2_peaks['indices']) / self.sample_rate)])\n",
        "            self.fig.add_trace(\n",
        "                go.Scatter(x=times_dt, y=audio_envelope[s2_peaks['indices']], mode='markers', name='S2 Beats',\n",
        "                           marker=dict(color='orange', symbol='circle', size=6),\n",
        "                           customdata=s2_peaks['customdata'],\n",
        "                           hovertemplate=hovertemplate), secondary_y=False)\n",
        "\n",
        "        if noise_peaks['indices']:\n",
        "            times_dt = pd.to_datetime([datetime.datetime.fromtimestamp(0) + datetime.timedelta(seconds=t) for t in\n",
        "                                       (np.array(noise_peaks['indices']) / self.sample_rate)])\n",
        "            self.fig.add_trace(\n",
        "                go.Scatter(x=times_dt, y=audio_envelope[noise_peaks['indices']], mode='markers', name='Noise/Rejected',\n",
        "                           marker=dict(color='grey', symbol='x', size=6),\n",
        "                           customdata=noise_peaks['customdata'],\n",
        "                           hovertemplate=hovertemplate), secondary_y=False)\n",
        "\n",
        "    def _add_bpm_hrv_traces(self, smoothed_bpm, analysis_data, windowed_hrv_df):\n",
        "        \"\"\"Adds BPM, BPM trend, and HRV traces.\"\"\"\n",
        "        if smoothed_bpm is not None and not smoothed_bpm.empty:\n",
        "            self.fig.add_trace(go.Scatter(x=smoothed_bpm.index, y=smoothed_bpm.values, name=\"Average BPM\", line=dict(color=\"#4a4a4a\", width=3)), secondary_y=True)\n",
        "\n",
        "        if \"long_term_bpm_series\" in analysis_data and not analysis_data[\"long_term_bpm_series\"].empty:\n",
        "            lt_series = analysis_data[\"long_term_bpm_series\"]\n",
        "            # Create datetime index for plotting\n",
        "            start_datetime = datetime.datetime.fromtimestamp(0)\n",
        "            lt_times_dt = pd.to_datetime([start_datetime + datetime.timedelta(seconds=t) for t in lt_series.index])\n",
        "            self.fig.add_trace(go.Scatter(\n",
        "                x=lt_times_dt,\n",
        "                y=lt_series.values,\n",
        "                name=\"BPM Trend (Belief)\",\n",
        "                line=dict(color='orange', width=2, dash='dot'),\n",
        "                visible='legendonly'),\n",
        "                secondary_y=True)\n",
        "        if windowed_hrv_df is not None and not windowed_hrv_df.empty and 'time' in windowed_hrv_df and 'rmssdc' in windowed_hrv_df and 'sdnn' in windowed_hrv_df:\n",
        "            hrv_times_dt = pd.to_datetime([datetime.datetime.fromtimestamp(0) + datetime.timedelta(seconds=t) for t in windowed_hrv_df['time']])\n",
        "            self.fig.add_trace(go.Scatter(x=hrv_times_dt, y=windowed_hrv_df['rmssdc'], name=\"RMSSDc\", line=dict(color='cyan', width=2), visible='legendonly'), secondary_y=True)\n",
        "            self.fig.add_trace(go.Scatter(x=hrv_times_dt, y=windowed_hrv_df['sdnn'], name=\"SDNN\", line=dict(color='magenta', width=2), visible='legendonly'), secondary_y=True)\n",
        "\n",
        "\n",
        "    def _add_annotations_and_summary(self, smoothed_bpm, hrv_summary, hrr_stats, peak_recovery_stats):\n",
        "        \"\"\"Adds min/max BPM annotations and the main summary box.\"\"\"\n",
        "        if smoothed_bpm is not None and not smoothed_bpm.empty:\n",
        "            max_bpm_val = smoothed_bpm.max()\n",
        "            min_bpm_val = smoothed_bpm.min()\n",
        "            max_bpm_time = smoothed_bpm.idxmax()\n",
        "            min_bpm_time = smoothed_bpm.idxmin()\n",
        "\n",
        "            # Add annotation for the maximum BPM\n",
        "            self.fig.add_annotation(x=max_bpm_time, y=max_bpm_val,\n",
        "                                    text=f\"Max: {max_bpm_val:.1f} BPM\",\n",
        "                                    showarrow=True, arrowhead=1, ax=20, ay=-40,\n",
        "                                    font=dict(color=\"#e36f6f\"), yref=\"y2\")\n",
        "\n",
        "            # Add annotation for the minimum BPM\n",
        "            self.fig.add_annotation(x=min_bpm_time, y=min_bpm_val,\n",
        "                                    text=f\"Min: {min_bpm_val:.1f} BPM\",\n",
        "                                    showarrow=True, arrowhead=1, ax=20, ay=40,\n",
        "                                    font=dict(color=\"#a3d194\"), yref=\"y2\")\n",
        "\n",
        "        if hrv_summary:\n",
        "            annotation_text = \"<b>Analysis Summary</b><br>\"\n",
        "            if hrv_summary.get('avg_bpm') is not None:\n",
        "                annotation_text += f\"Avg/Min/Max BPM: {hrv_summary['avg_bpm']:.1f} / {hrv_summary['min_bpm']:.1f} / {hrv_summary['max_bpm']:.1f}<br>\"\n",
        "            if hrr_stats and hrr_stats.get('hrr_value_bpm') is not None:\n",
        "                annotation_text += f\"<b>1-Min HRR: {hrr_stats['hrr_value_bpm']:.1f} BPM Drop</b><br>\"\n",
        "            if peak_recovery_stats and peak_recovery_stats.get('slope_bpm_per_sec') is not None:\n",
        "                annotation_text += f\"<b>Peak Recovery Rate: {peak_recovery_stats['slope_bpm_per_sec']:.2f} BPM/sec</b><br>\"\n",
        "            if hrv_summary.get('avg_rmssdc') is not None:\n",
        "                annotation_text += f\"Avg. Corrected RMSSD: {hrv_summary['avg_rmssdc']:.2f}<br>\"\n",
        "            if hrv_summary.get('avg_sdnn') is not None:\n",
        "                annotation_text += f\"Avg. Windowed SDNN: {hrv_summary['avg_sdnn']:.2f} ms\"\n",
        "\n",
        "            self.fig.add_annotation(text=annotation_text, align='left', showarrow=False,\n",
        "                                    xref='paper', yref='paper', x=0.02, y=0.98,\n",
        "                                    bordercolor='black', borderwidth=1,\n",
        "                                    bgcolor='rgba(255, 253, 231, 0.4)')\n",
        "\n",
        "    def _add_slope_traces(self, major_inclines, major_declines, peak_recovery_stats, peak_exertion_stats):\n",
        "        \"\"\"Adds traces for major exertion and recovery periods.\"\"\"\n",
        "        if major_inclines:\n",
        "            for i, incline in enumerate(major_inclines):\n",
        "                c_data = [incline['duration_sec'], incline['bpm_increase'], incline['slope_bpm_per_sec']]\n",
        "                self.fig.add_trace(go.Scatter(\n",
        "                    x=[incline['start_time'], incline['end_time']],\n",
        "                    y=[incline['start_bpm'], incline['end_bpm']],\n",
        "                    mode='lines', line=dict(color=\"purple\", width=4, dash=\"dash\"),\n",
        "                    name='Exertion', legendgroup='Exertion',\n",
        "                    showlegend=(i == 0), visible='legendonly', yaxis='y2',\n",
        "                    hovertemplate=\"<b>Exertion Period</b><br>Duration: %{customdata[0]:.1f}s<br>BPM Increase: %{customdata[1]:.1f}<br>Slope: %{customdata[2]:.2f} BPM/sec<extra></extra>\",\n",
        "                    customdata=np.array([c_data, c_data])))\n",
        "\n",
        "        if major_declines:\n",
        "            for i, decline in enumerate(major_declines):\n",
        "                c_data = [decline['duration_sec'], decline['bpm_decrease'], decline['slope_bpm_per_sec']]\n",
        "                self.fig.add_trace(go.Scatter(\n",
        "                    x=[decline['start_time'], decline['end_time']],\n",
        "                    y=[decline['start_bpm'], decline['end_bpm']],\n",
        "                    mode='lines', line=dict(color=\"#2ca02c\", width=4, dash=\"dash\"),\n",
        "                    name='Recovery', legendgroup='Recovery',\n",
        "                    showlegend=(i == 0), visible='legendonly', yaxis='y2',\n",
        "                    hovertemplate=\"<b>Recovery Period</b><br>Duration: %{customdata[0]:.1f}s<br>BPM Decrease: %{customdata[1]:.1f}<br>Slope: %{customdata[2]:.2f} BPM/sec<extra></extra>\",\n",
        "                    customdata=np.array([c_data, c_data])))\n",
        "\n",
        "\n",
        "        if peak_recovery_stats:\n",
        "            stats = peak_recovery_stats\n",
        "            self.fig.add_trace(go.Scatter(\n",
        "                x=[stats['start_time'], stats['end_time']],\n",
        "                y=[stats['start_bpm'], stats['end_bpm']],\n",
        "                mode='lines', line=dict(color=\"#ff69b4\", width=5, dash=\"solid\"),\n",
        "                name='Peak Recovery Slope', legendgroup='Steepest Slopes',\n",
        "                visible='legendonly', yaxis='y2',\n",
        "                hovertemplate=\"<b>Peak Recovery Slope</b><br>Slope: %{customdata[0]:.2f} BPM/sec<br>Duration: %{customdata[1]:.1f}s<extra></extra>\",\n",
        "                customdata=np.array([[stats['slope_bpm_per_sec'], stats['duration_sec']]]*2)))\n",
        "\n",
        "        if peak_exertion_stats:\n",
        "            stats = peak_exertion_stats\n",
        "            self.fig.add_trace(go.Scatter(\n",
        "                x=[stats['start_time'], stats['end_time']],\n",
        "                y=[stats['start_bpm'], stats['end_bpm']],\n",
        "                mode='lines', line=dict(color=\"#9d32a8\", width=5, dash=\"solid\"),\n",
        "                name='Peak Exertion Slope', legendgroup='Steepest Slopes',\n",
        "                visible='legendonly', yaxis='y2',\n",
        "                hovertemplate=\"<b>Peak Exertion Slope</b><br>Slope: +%{customdata[0]:.2f} BPM/sec<br>Duration: %{customdata[1]:.1f}s<extra></extra>\",\n",
        "                customdata=np.array([[stats['slope_bpm_per_sec'], stats['duration_sec']]]*2)))\n",
        "\n",
        "class ReportGenerator:\n",
        "    \"\"\"Handles the creation of text-based analysis reports.\"\"\"\n",
        "    def __init__(self, file_name: str, output_directory: str):\n",
        "        self.file_name = file_name\n",
        "        self.output_directory = output_directory\n",
        "        self.file_name_no_ext = os.path.splitext(file_name)[0]\n",
        "        self.base_name = os.path.basename(self.file_name_no_ext)\n",
        "\n",
        "    def save_analysis_settings(self, start_bpm_hint: Optional[float]):\n",
        "        \"\"\"Saves the user-configurable settings to a JSON file.\"\"\"\n",
        "        settings_path = os.path.join(self.output_directory, f\"{self.base_name}_Analysis_Settings.json\")\n",
        "        settings_to_save = {'start_bpm_hint': start_bpm_hint}\n",
        "        try:\n",
        "            with open(settings_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(settings_to_save, f, indent=4)\n",
        "            logging.info(f\"Analysis settings saved to {settings_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Could not save analysis settings file. Error: {e}\")\n",
        "\n",
        "    def save_analysis_summary(self, final_metrics: Dict):\n",
        "        \"\"\"Saves a comprehensive Markdown summary of the analysis results.\"\"\"\n",
        "        output_path = os.path.join(self.output_directory, f\"{self.base_name}_Analysis_Summary.md\")\n",
        "\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            self._write_summary_header(f)\n",
        "            self._write_overall_summary(f, final_metrics.get('hrv_summary'), final_metrics.get('hrr_stats'))\n",
        "            self._write_steepest_slopes(f, final_metrics.get('peak_exertion_stats'),\n",
        "                                        final_metrics.get('peak_recovery_stats'))\n",
        "            self._write_significant_changes(f, final_metrics.get('major_inclines'), final_metrics.get('major_declines'))\n",
        "            self._write_heartbeat_data_table(f, final_metrics.get('smoothed_bpm'), final_metrics.get('bpm_times'))\n",
        "\n",
        "        logging.info(f\"Markdown analysis summary saved to {output_path}\")\n",
        "\n",
        "    def create_chronological_log(self, audio_envelope: np.ndarray, sample_rate: int, all_raw_peaks: np.ndarray, analysis_data: Dict, final_metrics: Dict):\n",
        "        \"\"\"Creates a detailed, readable debug log file.\"\"\"\n",
        "        output_log_path = os.path.join(self.output_directory, f\"{self.base_name}_Debug_Log.md\")\n",
        "        logging.info(f\"Generating readable debug log at '{output_log_path}'...\")\n",
        "        merged_df = self._prepare_log_data(audio_envelope, sample_rate, all_raw_peaks, analysis_data, final_metrics.get('smoothed_bpm'), final_metrics.get('bpm_times'))\n",
        "        with open(output_log_path, \"w\", encoding=\"utf-8\") as log_file:\n",
        "            if merged_df is None or merged_df.empty:\n",
        "                log_file.write(\"# No significant events detected to log.\\n\")\n",
        "            else:\n",
        "                self._write_log_events(log_file, merged_df)\n",
        "        logging.info(\"Debug log generation complete.\")\n",
        "\n",
        "    def _prepare_log_data(self, audio_envelope, sample_rate, all_raw_peaks, analysis_data, smoothed_bpm, bpm_times):\n",
        "        \"\"\"Prepares and merges all data sources into a single DataFrame for logging.\"\"\"\n",
        "        events = []\n",
        "        debug_info = analysis_data.get('beat_debug_info', {})\n",
        "\n",
        "        for p in all_raw_peaks:\n",
        "            reason = debug_info.get(p)\n",
        "            if reason:\n",
        "                events.append({'time': p / sample_rate, 'type': 'Peak', 'amp': audio_envelope[p], 'reason': reason})\n",
        "        if 'trough_indices' in analysis_data:\n",
        "            for p in analysis_data['trough_indices']:\n",
        "                events.append({'time': p / sample_rate, 'type': 'Trough', 'amp': audio_envelope[p], 'reason': ''})\n",
        "\n",
        "        if not events: return None\n",
        "        events_df = pd.DataFrame(events).sort_values(by='time').set_index('time')\n",
        "\n",
        "        master_df = pd.DataFrame(index=np.arange(len(audio_envelope)) / sample_rate)\n",
        "        if 'dynamic_noise_floor_series' in analysis_data:\n",
        "            master_df['noise_floor'] = analysis_data['dynamic_noise_floor_series'].values\n",
        "        if smoothed_bpm is not None and not smoothed_bpm.empty:\n",
        "            smoothed_bpm_sec_index = pd.Series(data=smoothed_bpm.values, index=bpm_times).groupby(level=0).mean()\n",
        "            master_df['smoothed_bpm'] = smoothed_bpm_sec_index\n",
        "        if 'long_term_bpm_series' in analysis_data and not analysis_data['long_term_bpm_series'].empty:\n",
        "            master_df['lt_bpm'] = analysis_data['long_term_bpm_series'].groupby(level=0).mean()\n",
        "\n",
        "        master_df.ffill(inplace=True)\n",
        "\n",
        "        return pd.merge_asof(left=events_df, right=master_df, left_index=True,\n",
        "                             right_index=True, direction='nearest', tolerance=pd.Timedelta(seconds=0.5).total_seconds())\n",
        "\n",
        "    def _write_log_events(self, log_file, merged_df):\n",
        "        log_file.write(f\"# Chronological Debug Log for {os.path.basename(self.file_name)}\\n\")\n",
        "        log_file.write(f\"Analysis performed on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "        for row in merged_df.itertuples(name=\"LogEvent\"):\n",
        "            log_file.write(f\"## Time: `{row.Index:.4f}s`\\n\")\n",
        "\n",
        "            if row.type == 'Trough':\n",
        "                log_file.write(\"**Trough Detected**\\n\")\n",
        "            else:\n",
        "                raw_reason = getattr(row, 'reason', '')\n",
        "                if not raw_reason or raw_reason == 'Unknown':\n",
        "                    log_file.write(\"**Unclassified Peak**\\n\")\n",
        "                else:\n",
        "                    parts = raw_reason.split('§')\n",
        "                    final_peak_type, details_list = parts[0], parts[1:]\n",
        "                    log_file.write(f\"**{final_peak_type}.**\\n\")\n",
        "\n",
        "                    i = 0\n",
        "                    while i < len(details_list):\n",
        "                        tag = details_list[i]\n",
        "                        value = details_list[i + 1] if (i + 1) < len(details_list) else \"\"\n",
        "                        formatted_details = \"\"\n",
        "\n",
        "                        # MODIFICATION: Call the new standalone functions\n",
        "                        if \"PAIRING\" in tag:\n",
        "                            formatted_details = \"\\n\".join(format_pairing_details_list(value))\n",
        "                        elif \"LONE_S1_REJECT_REASON\" in tag:\n",
        "                            formatted_details = \"\\n\".join(format_lone_s1_details_list(value))\n",
        "                        elif \"LONE_S1_VALIDATE_REASON\" in tag:\n",
        "                            formatted_details = f\"- Lone S1 decision:\\n    - Validated: {value}\"\n",
        "                        elif \"ORIGINAL_REASON\" in tag:\n",
        "                            formatted_details = f\"- Original Classification:\\n    - `{value}`\"\n",
        "\n",
        "                        if formatted_details:\n",
        "                            log_file.write(f\"{formatted_details}\\n\")\n",
        "\n",
        "                        i += 2  # Move past the tag and its value\n",
        "\n",
        "            # Write all available metrics for every event type\n",
        "            metrics = {\n",
        "                \"Raw Amp\": getattr(row, 'amp', None),\n",
        "                \"Noise Floor\": getattr(row, 'noise_floor', None),\n",
        "                \"Average BPM (Smoothed)\": getattr(row, 'smoothed_bpm', None),\n",
        "                \"Long-Term BPM (Belief)\": getattr(row, 'lt_bpm', None)\n",
        "            }\n",
        "            for name, value in metrics.items():\n",
        "                if pd.notna(value):\n",
        "                    log_file.write(f\"- **{name}**: `{value:.1f}`\\n\")\n",
        "\n",
        "            log_file.write(\"\\n\\n\")\n",
        "\n",
        "    def _write_summary_header(self, f):\n",
        "        f.write(f\"# Analysis Report for: {os.path.basename(self.file_name)}\\n\")\n",
        "        f.write(f\"*Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\\n\")\n",
        "\n",
        "    def _write_overall_summary(self, f, hrv_summary, hrr_stats):\n",
        "        \"\"\"Writes the main summary table to the markdown report file.\"\"\"\n",
        "        f.write(\"## Overall Summary\\n\\n| Metric | Value |\\n|:---|:---|\\n\")\n",
        "        if hrv_summary:\n",
        "            if hrv_summary.get('avg_bpm') is not None:\n",
        "                f.write(f\"| **Average BPM** | {hrv_summary['avg_bpm']:.1f} BPM |\\n\")\n",
        "                f.write(f\"| **BPM Range** | {hrv_summary['min_bpm']:.1f} to {hrv_summary['max_bpm']:.1f} BPM |\\n\")\n",
        "            if hrv_summary.get('avg_rmssdc') is not None:\n",
        "                f.write(f\"| **Avg. Corrected RMSSD** | {hrv_summary['avg_rmssdc']:.2f} |\\n\")\n",
        "            if hrv_summary.get('avg_sdnn') is not None:\n",
        "                f.write(f\"| **Avg. Windowed SDNN** | {hrv_summary['avg_sdnn']:.2f} ms |\\n\")\n",
        "        if hrr_stats and hrr_stats.get('hrr_value_bpm') is not None:\n",
        "            f.write(f\"| **1-Minute HRR** | {hrr_stats['hrr_value_bpm']:.1f} BPM Drop |\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    def _write_steepest_slopes(self, f, peak_exertion_stats, peak_recovery_stats):\n",
        "        \"\"\"Writes the peak exertion and recovery slope data to the markdown report.\"\"\"\n",
        "        f.write(\"## Steepest Slopes Analysis\\n\\n### Peak Exertion (Fastest HR Increase)\\n\\n\")\n",
        "        if peak_exertion_stats:\n",
        "            pes = peak_exertion_stats\n",
        "            f.write(\"| Attribute | Value |\\n|:---|:---|\\n\")\n",
        "            f.write(f\"| **Rate** | `+{pes['slope_bpm_per_sec']:.2f}` BPM/second |\\n\")\n",
        "            f.write(f\"| **Period** | {pes['start_time'].strftime('%M:%S')} to {pes['end_time'].strftime('%M:%S')} |\\n\")\n",
        "            f.write(f\"| **Duration** | {pes['duration_sec']:.1f} seconds |\\n\")\n",
        "            f.write(f\"| **BPM Change** | {pes['start_bpm']:.1f} to {pes['end_bpm']:.1f} BPM |\\n\\n\")\n",
        "        else:\n",
        "            f.write(\"*No significant peak exertion period found.*\\n\\n\")\n",
        "\n",
        "        f.write(\"### Peak Recovery (Fastest HR Decrease)\\n\\n\")\n",
        "        if peak_recovery_stats:\n",
        "            prs = peak_recovery_stats\n",
        "            f.write(\"| Attribute | Value |\\n|:---|:---|\\n\")\n",
        "            f.write(f\"| **Rate** | `{prs['slope_bpm_per_sec']:.2f}` BPM/second |\\n\")\n",
        "            f.write(f\"| **Period** | {prs['start_time'].strftime('%M:%S')} to {prs['end_time'].strftime('%M:%S')} |\\n\")\n",
        "            f.write(f\"| **Duration** | {prs['duration_sec']:.1f} seconds |\\n\")\n",
        "            f.write(f\"| **BPM Change** | {prs['start_bpm']:.1f} to {prs['end_bpm']:.1f} BPM |\\n\\n\")\n",
        "        else:\n",
        "            f.write(\"*No significant peak recovery period found post-peak.*\\n\\n\")\n",
        "\n",
        "    def _write_significant_changes(self, f, major_inclines, major_declines):\n",
        "        \"\"\"Writes the sections on sustained heart rate increases and decreases to the report file.\"\"\"\n",
        "        f.write(\"## All Significant HR Changes\\n\\n### Exertion Periods (Sustained HR Increase)\\n\\n\")\n",
        "        if major_inclines:\n",
        "            epoch = datetime.datetime.fromtimestamp(0)\n",
        "            for incline in major_inclines:\n",
        "                # Calculate start and end times in seconds from the datetime objects\n",
        "                start_sec = (incline['start_time'] - epoch).total_seconds()\n",
        "                end_sec = (incline['end_time'] - epoch).total_seconds()\n",
        "                f.write(f\"- **From {start_sec:.1f}s to {end_sec:.1f}s:** Duration={incline['duration_sec']:.1f}s, Change=`+{incline['bpm_increase']:.1f}` BPM\\n\")\n",
        "        else:\n",
        "            f.write(\"*None found.*\\n\")\n",
        "\n",
        "        f.write(\"\\n### Recovery Periods (Sustained HR Decrease)\\n\\n\")\n",
        "        if major_declines:\n",
        "            epoch = datetime.datetime.fromtimestamp(0)\n",
        "            for decline in major_declines:\n",
        "                # Calculate start and end times in seconds from the datetime objects\n",
        "                start_sec = (decline['start_time'] - epoch).total_seconds()\n",
        "                end_sec = (decline['end_time'] - epoch).total_seconds()\n",
        "                f.write(f\"- **From {start_sec:.1f}s to {end_sec:.1f}s:** Duration={decline['duration_sec']:.1f}s, Change=`-{decline['bpm_decrease']:.1f}` BPM\\n\")\n",
        "        else:\n",
        "            f.write(\"*None found.*\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    def _write_heartbeat_data_table(self, f, smoothed_bpm, bpm_times):\n",
        "        \"\"\"Writes the final time-series BPM data to a markdown table in the report file.\"\"\"\n",
        "        f.write(\"## Heartbeat Data (BPM over Time)\\n\\n| Time (s) | Average BPM |\\n|:---:|:---:|\\n\")\n",
        "        if smoothed_bpm is not None and not smoothed_bpm.empty and bpm_times is not None:\n",
        "            # Use the raw numpy array of times for the table and match it with the smoothed BPM values\n",
        "            for t, bpm in zip(bpm_times, smoothed_bpm.values):\n",
        "                if not np.isnan(bpm):\n",
        "                    f.write(f\"| {t:.2f} | {bpm:.1f} |\\n\")\n",
        "        else:\n",
        "            f.write(\"| *No data* | *No data* |\\n\")\n",
        "\n",
        "# --- Standalone Utility & Pipeline Functions ---\n",
        "\n",
        "def convert_to_wav(file_path: str, target_path: str) -> bool:\n",
        "    \"\"\"Converts a given audio file to WAV format.\"\"\"\n",
        "    if not AudioSegment:\n",
        "        raise ImportError(\"Pydub/FFmpeg is required for audio conversion.\")\n",
        "\n",
        "    logging.info(f\"Converting {os.path.basename(file_path)} to WAV format...\")\n",
        "    try:\n",
        "        # Load the audio file\n",
        "        sound = AudioSegment.from_file(file_path)\n",
        "        # Convert to mono\n",
        "        sound = sound.set_channels(1)\n",
        "        # Export as WAV\n",
        "        sound.export(target_path, format=\"wav\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not convert file {file_path}. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def preprocess_audio(file_path: str, params: Dict, output_directory: str) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Reads, filters, and prepares the audio envelope for analysis.\"\"\"\n",
        "    downsample_factor = params['downsample_factor']\n",
        "    bandpass_freqs = params['bandpass_freqs']\n",
        "    save_debug_file = params['save_filtered_wav']\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        sample_rate, audio_data = wavfile.read(file_path)\n",
        "    if audio_data.ndim > 1:\n",
        "        audio_data = np.mean(audio_data, axis=1)\n",
        "\n",
        "    lowcut, highcut = bandpass_freqs\n",
        "\n",
        "    # Check if the downsample factor is too aggressive for the filter settings.\n",
        "    max_safe_downsample = int((sample_rate / (highcut * 2)) - 1)\n",
        "\n",
        "    if downsample_factor > max_safe_downsample:\n",
        "        logging.warning(\n",
        "            f\"Original 'downsample_factor' of {downsample_factor} is too high for a \"\n",
        "            f\"{highcut}Hz filter with a {sample_rate}Hz sample rate.\"\n",
        "        )\n",
        "        downsample_factor = max(1, max_safe_downsample)\n",
        "        logging.warning(f\"Adjusting 'downsample_factor' to a safe value of {downsample_factor}.\")\n",
        "\n",
        "    if downsample_factor > 1:\n",
        "        new_sample_rate = sample_rate // downsample_factor\n",
        "        audio_downsampled = audio_data[::downsample_factor]\n",
        "    else:\n",
        "        new_sample_rate = sample_rate\n",
        "        audio_downsampled = audio_data\n",
        "\n",
        "    nyquist = 0.5 * new_sample_rate\n",
        "    low, high = lowcut / nyquist, highcut / nyquist\n",
        "\n",
        "    if high >= 1.0:\n",
        "        raise ValueError(f\"Cannot create a {highcut}Hz filter. The effective sample rate of {new_sample_rate}Hz is too low.\")\n",
        "\n",
        "    b, a = butter(2, [low, high], btype='band')\n",
        "    audio_filtered = filtfilt(b, a, audio_downsampled)\n",
        "\n",
        "    if save_debug_file:\n",
        "        debug_path = f\"{os.path.splitext(file_path)[0]}_filtered_debug.wav\"\n",
        "        normalized_audio = np.int16(audio_filtered / np.max(np.abs(audio_filtered)) * 32767)\n",
        "        wavfile.write(debug_path, new_sample_rate, normalized_audio)\n",
        "\n",
        "    audio_abs = np.abs(audio_filtered)\n",
        "    window_size = new_sample_rate // 10\n",
        "    audio_envelope = pd.Series(audio_abs).rolling(window=window_size, min_periods=1, center=True).mean().values\n",
        "\n",
        "    if params['save_filtered_wav']:\n",
        "        base_name = os.path.basename(os.path.splitext(file_path)[0])\n",
        "        debug_path = os.path.join(output_directory, f\"{base_name}_filtered_debug.wav\")\n",
        "        normalized_audio = np.int16(audio_filtered / np.max(np.abs(audio_filtered)) * 32767)\n",
        "        wavfile.write(debug_path, new_sample_rate, normalized_audio)\n",
        "\n",
        "    return audio_envelope, new_sample_rate\n",
        "\n",
        "def _calculate_dynamic_noise_floor(audio_envelope: np.ndarray, sample_rate: int, params: Dict) -> Tuple[pd.Series, np.ndarray]:\n",
        "    \"\"\"Calculates a dynamic noise floor based on a sanitized set of audio troughs.\"\"\"\n",
        "    min_peak_dist_samples = int(params['min_peak_distance_sec'] * sample_rate)\n",
        "    trough_prom_thresh = np.quantile(audio_envelope, params['trough_prominence_quantile'])\n",
        "\n",
        "    # --- STEP 1: Find all potential troughs initially ---\n",
        "    all_trough_indices, _ = find_peaks(-audio_envelope, distance=min_peak_dist_samples, prominence=trough_prom_thresh)\n",
        "    logging.info(f\"Trough detection: min_peak_distance_sec={params['min_peak_distance_sec']}s -> {min_peak_dist_samples} samples, found {len(all_trough_indices)} initial troughs\")\n",
        "\n",
        "    # If we don't have enough troughs to begin with, fall back to a simple static floor.\n",
        "    if len(all_trough_indices) < 5:\n",
        "        logging.warning(\"Not enough troughs found for sanitization. Using a static noise floor.\")\n",
        "        fallback_value = np.quantile(audio_envelope, params['noise_floor_quantile'])\n",
        "        dynamic_noise_floor = pd.Series(fallback_value, index=np.arange(len(audio_envelope)))\n",
        "        return dynamic_noise_floor, all_trough_indices\n",
        "\n",
        "    # --- STEP 2: Create a preliminary 'draft' noise floor from ALL troughs ---\n",
        "    # This draft version is used only to evaluate the troughs themselves.\n",
        "    trough_series_draft = pd.Series(index=all_trough_indices, data=audio_envelope[all_trough_indices])\n",
        "    dense_troughs_draft = trough_series_draft.reindex(np.arange(len(audio_envelope))).interpolate()\n",
        "    noise_window_samples = int(params['noise_window_sec'] * sample_rate)\n",
        "    quantile_val = params['noise_floor_quantile']\n",
        "    draft_noise_floor = dense_troughs_draft.rolling(window=noise_window_samples, min_periods=3, center=True).quantile(quantile_val)\n",
        "    draft_noise_floor = draft_noise_floor.bfill().ffill() # Fill any gaps\n",
        "\n",
        "    # --- STEP 3: Sanitize the trough list ---\n",
        "    # remove any toughs too far from the noise floor\n",
        "    sanitized_trough_indices = []\n",
        "    rejection_multiplier = params.get('trough_rejection_multiplier', 4.0)\n",
        "    for trough_idx in all_trough_indices:\n",
        "        trough_amp = audio_envelope[trough_idx]\n",
        "        floor_at_trough = draft_noise_floor.iloc[trough_idx]\n",
        "        # Keep the trough only if it's not too high above the draft floor\n",
        "        if not pd.isna(floor_at_trough) and trough_amp <= (rejection_multiplier * floor_at_trough):\n",
        "            sanitized_trough_indices.append(trough_idx)\n",
        "\n",
        "    logging.info(f\"Trough Sanitization: Kept {len(sanitized_trough_indices)} of {len(all_trough_indices)} initial troughs.\")\n",
        "\n",
        "    # --- STEP 4: Calculate more accurate noise floor using only sanitized troughs ---\n",
        "    if len(sanitized_trough_indices) > 2:\n",
        "        trough_series_final = pd.Series(index=sanitized_trough_indices, data=audio_envelope[sanitized_trough_indices])\n",
        "        dense_troughs_final = trough_series_final.reindex(np.arange(len(audio_envelope))).interpolate()\n",
        "        dynamic_noise_floor = dense_troughs_final.rolling(window=noise_window_samples, min_periods=3, center=True).quantile(quantile_val)\n",
        "        dynamic_noise_floor = dynamic_noise_floor.bfill().ffill()\n",
        "    else:\n",
        "        # If sanitization removed too many troughs, it's safer to use the original draft floor.\n",
        "        logging.warning(\"Not enough sanitized troughs remaining. Using non-sanitized floor as fallback.\")\n",
        "        dynamic_noise_floor = draft_noise_floor\n",
        "\n",
        "    # Final check for any remaining null values\n",
        "    if dynamic_noise_floor.isnull().all():\n",
        "         fallback_val = np.quantile(audio_envelope, 0.1)\n",
        "         dynamic_noise_floor = pd.Series(fallback_val, index=np.arange(len(audio_envelope)))\n",
        "\n",
        "    return dynamic_noise_floor, np.array(sanitized_trough_indices)\n",
        "\n",
        "\n",
        "def calculate_blended_confidence(deviation: float, bpm: float, params: Dict) -> float:\n",
        "    \"\"\"\n",
        "    Calculates a confidence score for pairing two peaks based on amplitude deviation.\n",
        "    This version dynamically constructs the confidence curve based on the current BPM\n",
        "    to reflect physiological expectations (heart's contractility).\n",
        "    \"\"\"\n",
        "    # Get the anchor points for our dynamic model from params\n",
        "    bpm_points = [params['contractility_bpm_low'], params['contractility_bpm_high']]\n",
        "    deviation_points = params['confidence_deviation_points']\n",
        "\n",
        "    # Get the two boundary curves (for low and high BPM)\n",
        "    curve_low = np.array(params['confidence_curve_low_bpm'])\n",
        "    curve_high = np.array(params['confidence_curve_high_bpm'])\n",
        "\n",
        "    # --- Create the Live Confidence Curve ---\n",
        "    # Calculate how far the current BPM is into the transition zone (0.0 to 1.0)\n",
        "    blend_ratio = np.clip((bpm - bpm_points[0]) / (bpm_points[1] - bpm_points[0]), 0, 1)\n",
        "\n",
        "    # Linearly interpolate between the low and high curves to get the live curve\n",
        "    live_confidence_curve = curve_low + (curve_high - curve_low) * blend_ratio\n",
        "\n",
        "    final_confidence = np.interp(deviation, deviation_points, live_confidence_curve)\n",
        "\n",
        "    return final_confidence\n",
        "\n",
        "\n",
        "def _get_peak_strength(peak_idx: int, audio_envelope: np.ndarray, dynamic_noise_floor: pd.Series) -> float:\n",
        "    \"\"\"Calculates a peak's strength (amplitude above the noise floor).\"\"\"\n",
        "    return max(0, audio_envelope[peak_idx] - dynamic_noise_floor.iloc[peak_idx])\n",
        "\n",
        "def _adjust_confidence_with_stability_and_ratio(confidence: float, s1_idx: int, s2_idx: int, audio_envelope: np.ndarray, dynamic_noise_floor: pd.Series,\n",
        "                                               long_term_bpm: float, pairing_ratio: float, params: Dict, sample_rate: int,\n",
        "                                               peak_bpm_time_sec: Optional[float], recovery_end_time_sec: Optional[float], beat_count: int) -> Tuple[float, str]:\n",
        "    \"\"\"Applies a full suite of confidence adjustments based on rhythm stability and S1/S2 strength ratio.\"\"\"\n",
        "    reason = \"\"\n",
        "\n",
        "    # --- 1. Universal Stability Pre-Adjustment ---\n",
        "    if beat_count >= 5:\n",
        "        floor = params.get(\"stability_confidence_floor\", 0.85)\n",
        "        ceiling = params.get(\"stability_confidence_ceiling\", 1.10)\n",
        "        stability_factor = np.interp(pairing_ratio, [0.0, 1.0], [floor, ceiling])\n",
        "        confidence *= stability_factor\n",
        "        reason += f\"\\n- Stability Pre-Adjust: x{stability_factor:.2f} (Pairing Ratio: {pairing_ratio:.0%})\"\n",
        "\n",
        "    # --- 2. Calculate Peak Strengths and Expected Ratio ---\n",
        "    s1_strength = _get_peak_strength(s1_idx, audio_envelope, dynamic_noise_floor)\n",
        "    s2_strength = _get_peak_strength(s2_idx, audio_envelope, dynamic_noise_floor)\n",
        "    current_s2_s1_strength_ratio = s2_strength / (s1_strength + 1e-9)\n",
        "\n",
        "    # Determine expected ratio based on BPM and recovery state\n",
        "    is_in_recovery = (peak_bpm_time_sec is not None and recovery_end_time_sec is not None and\n",
        "                      peak_bpm_time_sec < (s1_idx / sample_rate) < recovery_end_time_sec)\n",
        "    effective_bpm = max(long_term_bpm, params['contractility_bpm_low']) if is_in_recovery else long_term_bpm\n",
        "    max_expected_s2_s1_ratio = np.interp(effective_bpm,\n",
        "                                       [params['contractility_bpm_low'], params['contractility_bpm_high']],\n",
        "                                       [params['s2_s1_ratio_low_bpm'], params['s2_s1_ratio_high_bpm']])\n",
        "\n",
        "    # --- 3. Apply Final Dynamic Boost or Penalty Amount ---\n",
        "    if current_s2_s1_strength_ratio > max_expected_s2_s1_ratio:\n",
        "        # PENALTY is scaled by the severity of the violation.\n",
        "        min_penalty = params.get(\"penalty_amount_min\", 0.15)\n",
        "        max_penalty = params.get(\"penalty_amount_max\", 0.40)\n",
        "        violation_severity = current_s2_s1_strength_ratio / max_expected_s2_s1_ratio\n",
        "        severity_scale = np.clip((violation_severity - 1.0) / 2.0, 0, 1)\n",
        "        penalty_range = max_penalty - min_penalty\n",
        "        penalty_amount = min_penalty + (severity_scale * penalty_range)\n",
        "        confidence -= penalty_amount\n",
        "        reason += f\"\\n- PENALIZED by {penalty_amount:.2f} (S2 Str. Ratio {current_s2_s1_strength_ratio:.1f}x > Expected {max_expected_s2_s1_ratio:.1f}x)\"\n",
        "\n",
        "    elif s1_strength > (s2_strength * params.get('s1_s2_boost_ratio', 1.2)):\n",
        "        # BOOST is now also scaled by severity.\n",
        "        min_boost = params.get(\"boost_amount_min\", 0.10)\n",
        "        max_boost = params.get(\"boost_amount_max\", 0.35)\n",
        "        actual_s1_s2_ratio = s1_strength / (s2_strength + 1e-9)\n",
        "        boost_threshold_ratio = params.get('s1_s2_boost_ratio', 1.2)\n",
        "        exceedance_scale = np.clip((actual_s1_s2_ratio - boost_threshold_ratio) / (4.0 - boost_threshold_ratio), 0, 1)\n",
        "        boost_range = max_boost - min_boost\n",
        "        boost_amount = min_boost + (exceedance_scale * boost_range)\n",
        "        confidence += boost_amount\n",
        "        reason += f\"\\n- BOOSTED by {boost_amount:.2f} (S1 Str. Ratio {actual_s1_s2_ratio:.1f}x > S2)\"\n",
        "\n",
        "    return max(0.0, min(1.0, confidence)), reason\n",
        "\n",
        "def _apply_interval_penalty(confidence: float, interval_sec: float, s1_s2_max_interval: float, params: Dict) -> Tuple[float, str]:\n",
        "    \"\"\"\n",
        "    Applies a graduated penalty to the confidence score if the S1-S2 interval is too long.\n",
        "    Returns the adjusted confidence and a reason string.\n",
        "    \"\"\"\n",
        "    # If the feature is disabled or the interval is within the allowed maximum, do nothing.\n",
        "    if not params.get(\"enable_interval_penalty\", True) or interval_sec <= s1_s2_max_interval:\n",
        "        return confidence, \"\"\n",
        "\n",
        "    start_factor = params.get(\"interval_penalty_start_factor\", 1.0)\n",
        "    full_factor = params.get(\"interval_penalty_full_factor\", 1.4)\n",
        "    max_penalty = params.get(\"interval_max_penalty\", 0.75)\n",
        "\n",
        "    # Define the range where the penalty is applied.\n",
        "    penalty_zone_start = s1_s2_max_interval * start_factor\n",
        "    penalty_zone_end = s1_s2_max_interval * full_factor\n",
        "\n",
        "    if interval_sec <= penalty_zone_start:\n",
        "        return confidence, \"\"\n",
        "\n",
        "    # Calculate how far into the \"penalty zone\" the interval is (from 0.0 to 1.0).\n",
        "    exceedance_scale = (interval_sec - penalty_zone_start) / (penalty_zone_end - penalty_zone_start + 1e-9)\n",
        "    exceedance_scale = np.clip(exceedance_scale, 0, 1)\n",
        "\n",
        "    # The penalty is scaled linearly across the zone.\n",
        "    penalty_amount = exceedance_scale * max_penalty\n",
        "    adjusted_confidence = max(0, confidence - penalty_amount)\n",
        "\n",
        "    penalty_reason = f\"\\n- Interval PENALTY by {penalty_amount:.2f} (Interval {interval_sec:.3f}s > Max {s1_s2_max_interval:.3f}s)\"\n",
        "\n",
        "    return adjusted_confidence, penalty_reason\n",
        "\n",
        "def calculate_lone_s1_confidence(current_peak_idx: int, last_s1_idx: int, long_term_bpm: float, audio_envelope: np.ndarray,\n",
        "                                 dynamic_noise_floor: pd.Series, sample_rate: int, params: Dict) -> Tuple[float, str]:\n",
        "    \"\"\"\n",
        "    Calculates a confidence score for a Lone S1 candidate based on a weighted average of\n",
        "    its rhythmic timing and its amplitude consistency with the previous beat.\n",
        "    \"\"\"\n",
        "    # --- 1. Calculate Rhythmic Fit Score ---\n",
        "    expected_rr_sec = 60.0 / long_term_bpm\n",
        "    actual_rr_sec = (current_peak_idx - last_s1_idx) / sample_rate\n",
        "    rhythm_deviation_pct = abs(actual_rr_sec - expected_rr_sec) / expected_rr_sec\n",
        "\n",
        "    rhythm_score = np.interp(\n",
        "        rhythm_deviation_pct,\n",
        "        params['lone_s1_rhythm_deviation_points'],\n",
        "        params['lone_s1_rhythm_confidence_curve']\n",
        "    )\n",
        "    rhythm_reason = f\"Rhythm Fit={rhythm_score:.2f} (Interval {actual_rr_sec:.3f}s vs Expected {expected_rr_sec:.3f}s)\"\n",
        "\n",
        "    # --- 2. Calculate Amplitude Fit Score ---\n",
        "    last_s1_strength = _get_peak_strength(last_s1_idx, audio_envelope, dynamic_noise_floor)\n",
        "    current_peak_strength = _get_peak_strength(current_peak_idx, audio_envelope, dynamic_noise_floor)\n",
        "    amplitude_ratio = current_peak_strength / (last_s1_strength + 1e-9)\n",
        "\n",
        "    amplitude_score = np.interp(\n",
        "        amplitude_ratio,\n",
        "        params['lone_s1_amplitude_ratio_points'],\n",
        "        params['lone_s1_amplitude_confidence_curve']\n",
        "    )\n",
        "    amplitude_reason = f\"Amplitude Fit={amplitude_score:.2f} (Strength Ratio {amplitude_ratio:.2f}x)\"\n",
        "\n",
        "    # --- 3. Combine Scores with Weights ---\n",
        "    rhythm_weight = params['lone_s1_rhythm_weight']\n",
        "    amplitude_weight = params['lone_s1_amplitude_weight']\n",
        "    final_confidence = (rhythm_score * rhythm_weight) + (amplitude_score * amplitude_weight)\n",
        "\n",
        "    reason_str = f\"{rhythm_reason}, {amplitude_reason}\"\n",
        "    return final_confidence, reason_str\n",
        "\n",
        "def update_long_term_bpm(new_rr_sec: float, current_long_term_bpm: float, params: Dict) -> float:\n",
        "    \"\"\"Updates the long-term BPM belief based on a new R-R interval.\"\"\"\n",
        "    instant_bpm = 60.0 / new_rr_sec\n",
        "    lr = params['long_term_bpm_learning_rate']\n",
        "    max_change_per_beat = params['max_bpm_change_per_beat']\n",
        "\n",
        "    # Calculate the target BPM using an exponential moving average\n",
        "    target_bpm = ((1 - lr) * current_long_term_bpm) + (lr * instant_bpm)\n",
        "\n",
        "    # Limit how much the BPM can change in a single beat (a \"speed limiter\")\n",
        "    max_change = max_change_per_beat * new_rr_sec # Scale limit by interval duration\n",
        "    proposed_change = target_bpm - current_long_term_bpm\n",
        "    limited_change = np.clip(proposed_change, -max_change, max_change)\n",
        "\n",
        "    # Apply the limited change and enforce absolute min/max BPM boundaries\n",
        "    new_bpm = current_long_term_bpm + limited_change\n",
        "    return max(params['min_bpm'], min(new_bpm, params['max_bpm']))\n",
        "\n",
        "def correct_peaks_by_rhythm(peaks: np.ndarray, audio_envelope: np.ndarray, sample_rate: int, params: Dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Refines a list of S1 peaks by removing rhythmically implausible beats.\n",
        "    If two beats are too close together, the one with the lower amplitude is discarded.\n",
        "    \"\"\"\n",
        "    # If we have too few peaks, correction is unreliable and unnecessary.\n",
        "    if len(peaks) < 5:\n",
        "        return peaks\n",
        "\n",
        "    logging.info(f\"--- STAGE 4: Correcting peaks based on rhythm. Initial count: {len(peaks)} ---\")\n",
        "\n",
        "    # Calculate the median R-R interval to establish a stable rhythmic expectation.\n",
        "    rr_intervals_sec = np.diff(peaks) / sample_rate\n",
        "    median_rr_sec = np.median(rr_intervals_sec)\n",
        "\n",
        "    # Any interval shorter than this threshold is considered a conflict.\n",
        "    correction_threshold_sec = median_rr_sec * params.get(\"rr_correction_threshold_pct\", 0.6)\n",
        "    logging.info(f\"Median R-R: {median_rr_sec:.3f}s. Correction threshold: {correction_threshold_sec:.3f}s.\")\n",
        "\n",
        "    # We build a new list of corrected peaks. Start with the first peak as a given.\n",
        "    corrected_peaks = [peaks[0]]\n",
        "\n",
        "    # Iterate through the original peaks, starting from the second one.\n",
        "    for i in range(1, len(peaks)):\n",
        "        current_peak = peaks[i]\n",
        "        last_accepted_peak = corrected_peaks[-1]\n",
        "        interval_sec = (current_peak - last_accepted_peak) / sample_rate\n",
        "        if interval_sec < correction_threshold_sec:\n",
        "            # CONFLICT: The current peak is too close to the last accepted one.\n",
        "            # We must decide which one to keep. The one with the higher amplitude wins.\n",
        "            last_peak_amp = audio_envelope[last_accepted_peak]\n",
        "            current_peak_amp = audio_envelope[current_peak]\n",
        "            if current_peak_amp > last_peak_amp:\n",
        "                # The current peak is stronger, so it REPLACES the last accepted peak.\n",
        "                logging.info(f\"Conflict at {current_peak/sample_rate:.2f}s. Replaced previous peak at {last_accepted_peak/sample_rate:.2f}s due to higher amplitude.\")\n",
        "                corrected_peaks[-1] = current_peak\n",
        "            else:\n",
        "                # The last accepted peak was stronger, so we DISCARD the current peak.\n",
        "                logging.info(f\"Conflict at {current_peak/sample_rate:.2f}s. Discarding current peak due to lower amplitude.\")\n",
        "                pass  # Do nothing, effectively dropping the current_peak\n",
        "        else:\n",
        "            # NO CONFLICT: The interval is plausible. Add the peak to our corrected list.\n",
        "            corrected_peaks.append(current_peak)\n",
        "\n",
        "    final_peak_count = len(corrected_peaks)\n",
        "    if final_peak_count < len(peaks):\n",
        "        logging.info(f\"Correction complete. Removed {len(peaks) - final_peak_count} peak(s). Final count: {final_peak_count}\")\n",
        "    else:\n",
        "        logging.info(\"Correction pass complete. No rhythmic conflicts found.\")\n",
        "    return np.array(corrected_peaks)\n",
        "\n",
        "\n",
        "def _fix_rhythmic_discontinuities(s1_peaks: np.ndarray, all_raw_peaks: np.ndarray, debug_info: Dict,\n",
        "                                  audio_envelope: np.ndarray, dynamic_noise_floor: pd.Series, params: Dict,\n",
        "                                  sample_rate: int) -> Tuple[np.ndarray, Dict, int]:\n",
        "    \"\"\"\n",
        "    Identifies and attempts to fix rhythmic discontinuities by re-evaluating misclassified peaks.\n",
        "    \"\"\"\n",
        "    log_level = params.get(\"correction_log_level\", \"INFO\").upper()\n",
        "\n",
        "    def log_debug(msg):\n",
        "        if log_level == \"DEBUG\":\n",
        "            logging.info(f\"[Correction DEBUG] {msg}\")\n",
        "\n",
        "    margin = 3\n",
        "    if len(s1_peaks) < margin * 2:\n",
        "        log_debug(f\"Skipping correction pass: Not enough S1 peaks ({len(s1_peaks)}) to apply a margin of {margin}.\")\n",
        "        return s1_peaks, debug_info, 0\n",
        "\n",
        "    rr_intervals_sec = np.diff(s1_peaks) / sample_rate\n",
        "    q1, q3 = np.percentile(rr_intervals_sec, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    stable_rr_intervals = rr_intervals_sec[\n",
        "        (rr_intervals_sec > (q1 - 1.5 * iqr)) & (rr_intervals_sec < (q3 + 1.5 * iqr))]\n",
        "\n",
        "    if len(stable_rr_intervals) < 1:\n",
        "        log_debug(\"Not enough stable R-R intervals to determine median. Skipping correction.\")\n",
        "        return s1_peaks, debug_info, 0\n",
        "\n",
        "    median_rr_sec = np.median(stable_rr_intervals)\n",
        "    short_conflict_threshold_sec = median_rr_sec * params[\"rr_correction_threshold_pct\"]\n",
        "    long_conflict_threshold_sec = median_rr_sec * params.get(\"rr_correction_long_interval_pct\", 1.7)\n",
        "\n",
        "    log_debug(\n",
        "        f\"Median R-R: {median_rr_sec:.3f}s. Short Threshold: < {short_conflict_threshold_sec:.3f}s. Long Threshold: > {long_conflict_threshold_sec:.3f}s.\")\n",
        "\n",
        "    corrected_debug_info = debug_info.copy()\n",
        "    peaks_to_add = set()\n",
        "    corrections_made = 0\n",
        "\n",
        "    # --- Pass 1: Look for LONG intervals (missed beats) ---\n",
        "    log_debug(f\"Checking for long intervals between beat {margin} and beat {len(s1_peaks) - margin}...\")\n",
        "    for i in range(margin, len(s1_peaks) - 1 - margin):\n",
        "        s1_start_idx, s1_end_idx = s1_peaks[i], s1_peaks[i + 1]\n",
        "        if (s1_end_idx - s1_start_idx) / sample_rate > long_conflict_threshold_sec:\n",
        "            log_debug(f\"Found LONG interval at {s1_start_idx / sample_rate:.2f}s. Investigating gap...\")\n",
        "            gap_candidates = [p for p in all_raw_peaks if\n",
        "                              s1_start_idx < p < s1_end_idx and \"Noise\" in debug_info.get(p, \"\")]\n",
        "            for candidate_s1 in gap_candidates:\n",
        "                if candidate_s1 in peaks_to_add: continue\n",
        "                current_raw_idx = np.searchsorted(all_raw_peaks, candidate_s1)\n",
        "                if current_raw_idx + 1 >= len(all_raw_peaks): continue\n",
        "                candidate_s2 = all_raw_peaks[current_raw_idx + 1]\n",
        "                if candidate_s2 >= s1_end_idx or \"Noise\" not in debug_info.get(candidate_s2, \"\"): continue\n",
        "\n",
        "                s1_strength = _get_peak_strength(candidate_s1, audio_envelope, dynamic_noise_floor)\n",
        "                is_strong_s1 = s1_strength > (\n",
        "                            params[\"penalty_waiver_strength_ratio\"] * dynamic_noise_floor.iloc[candidate_s1])\n",
        "                is_ratio_plausible = (audio_envelope[candidate_s2] / (audio_envelope[candidate_s1] + 1e-9)) < params[\n",
        "                    \"penalty_waiver_max_s2_s1_ratio\"]\n",
        "\n",
        "                if is_strong_s1 and is_ratio_plausible:\n",
        "                    log_debug(f\"  - SUCCESS: Re-labeling S1/S2 pair at {candidate_s1 / sample_rate:.2f}s.\")\n",
        "                    corrections_made += 1\n",
        "                    peaks_to_add.add(candidate_s1)\n",
        "                    original_reason_s1 = corrected_debug_info.get(candidate_s1, \"Noise\")\n",
        "                    corrected_debug_info[\n",
        "                        candidate_s1] = f\"{PeakType.S1_CORRECTED_GAP.value}§ORIGINAL_REASON§{original_reason_s1}\"\n",
        "                    original_reason_s2 = corrected_debug_info.get(candidate_s2, \"Noise\")\n",
        "                    corrected_debug_info[\n",
        "                        candidate_s2] = f\"{PeakType.S2_CORRECTED_GAP.value}§ORIGINAL_REASON§{original_reason_s2}\"\n",
        "                    break\n",
        "\n",
        "    # --- Pass 2: Look for SHORT intervals (adjacent S1s) ---\n",
        "    temp_s1_list = sorted(list(set(s1_peaks) | peaks_to_add))\n",
        "    peaks_to_remove = set()\n",
        "    log_debug(\"Starting SHORT interval check...\")\n",
        "\n",
        "    # Correctly iterate and compare adjacent beats\n",
        "    for i in range(margin, len(temp_s1_list) - 1 - margin):\n",
        "        beat_A_idx = temp_s1_list[i]\n",
        "        beat_B_idx = temp_s1_list[i + 1]\n",
        "\n",
        "        # Skip if either beat has already been marked for removal\n",
        "        if beat_A_idx in peaks_to_remove or beat_B_idx in peaks_to_remove:\n",
        "            continue\n",
        "\n",
        "        interval_sec = (beat_B_idx - beat_A_idx) / sample_rate\n",
        "        if interval_sec < short_conflict_threshold_sec:\n",
        "            log_debug(\n",
        "                f\"Found SHORT interval of {interval_sec:.3f}s between beats at {beat_A_idx / sample_rate:.2f}s and {beat_B_idx / sample_rate:.2f}s. Resolving...\")\n",
        "\n",
        "            # Decide which beat to remove based on amplitude\n",
        "            amp_A = audio_envelope[beat_A_idx]\n",
        "            amp_B = audio_envelope[beat_B_idx]\n",
        "\n",
        "            if amp_B > amp_A:\n",
        "                peaks_to_remove.add(beat_A_idx)\n",
        "                log_debug(f\"  - Removing weaker peak at {beat_A_idx / sample_rate:.2f}s.\")\n",
        "                corrections_made += 1\n",
        "            else:\n",
        "                peaks_to_remove.add(beat_B_idx)\n",
        "                log_debug(f\"  - Removing weaker peak at {beat_B_idx / sample_rate:.2f}s.\")\n",
        "                corrections_made += 1\n",
        "\n",
        "    # Construct the final list of S1 peaks after all corrections\n",
        "    final_s1_peaks = [p for p in temp_s1_list if p not in peaks_to_remove]\n",
        "\n",
        "    return np.array(sorted(final_s1_peaks)), corrected_debug_info, corrections_made\n",
        "\n",
        "def calculate_windowed_hrv(s1_peaks: np.ndarray, sample_rate: int, params: Dict) -> pd.DataFrame:\n",
        "    \"\"\" Calculates HRV metrics using R-R intervals based on changing heart rate \"\"\"\n",
        "    window_size_beats = params['hrv_window_size_beats']\n",
        "    step_size_beats = params['hrv_step_size_beats']\n",
        "\n",
        "    # First, calculate all R-R intervals from the S1 peaks\n",
        "    if len(s1_peaks) < window_size_beats:\n",
        "        logging.warning(f\"Not enough beats ({len(s1_peaks)}) to perform windowed HRV analysis with a window of {window_size_beats} beats.\")\n",
        "        return pd.DataFrame(columns=['time', 'rmssdc', 'sdnn', 'bpm'])\n",
        "\n",
        "    rr_intervals_sec = np.diff(s1_peaks) / sample_rate\n",
        "    s1_times_sec = s1_peaks / sample_rate\n",
        "\n",
        "    results = []\n",
        "    # Iterate through the R-R intervals with a sliding window\n",
        "    for i in range(0, len(rr_intervals_sec) - window_size_beats + 1, step_size_beats):\n",
        "        window_rr_sec = rr_intervals_sec[i : i + window_size_beats]\n",
        "        window_rr_ms = window_rr_sec * 1000\n",
        "        start_time = s1_times_sec[i]\n",
        "        end_time = s1_times_sec[i + window_size_beats]\n",
        "        window_mid_time = (start_time + end_time) / 2.0\n",
        "\n",
        "        # --- Calculate HRV Metrics for the Window ---\n",
        "        mean_rr_ms = np.mean(window_rr_ms)\n",
        "        sdnn = np.std(window_rr_ms)\n",
        "        successive_diffs_ms = np.diff(window_rr_ms)\n",
        "        rmssd = np.sqrt(np.mean(successive_diffs_ms**2))\n",
        "\n",
        "        # --- Calculate Corrected RMSSD (RMSSDc) ---\n",
        "        mean_rr_sec = mean_rr_ms / 1000.0\n",
        "        rmssdc = rmssd / mean_rr_sec if mean_rr_sec > 0 else 0\n",
        "\n",
        "        # Calculate the average BPM within this specific window\n",
        "        window_bpm = 60 / mean_rr_sec if mean_rr_sec > 0 else 0\n",
        "\n",
        "        results.append({\n",
        "            'time': window_mid_time,\n",
        "            'rmssdc': rmssdc,\n",
        "            'sdnn': sdnn,\n",
        "            'bpm': window_bpm\n",
        "        })\n",
        "\n",
        "    if not results:\n",
        "        logging.warning(\"Could not perform windowed HRV analysis. Recording may be too short or have too few beats.\")\n",
        "        return pd.DataFrame(columns=['time', 'rmssdc', 'sdnn', 'bpm'])\n",
        "\n",
        "    logging.info(f\"Beat-based windowed HRV analysis complete. Generated {len(results)} data points.\")\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def calculate_bpm_series(peaks: np.ndarray, sample_rate: int, params: Dict) -> Tuple[pd.Series, np.ndarray]:\n",
        "    \"\"\"Calculates and smooths the final BPM series from S1 peaks.\"\"\"\n",
        "    if len(peaks) < 2: return pd.Series(dtype=np.float64), np.array([])\n",
        "    peak_times = peaks / sample_rate\n",
        "    time_diffs = np.diff(peak_times)\n",
        "    valid_diffs = time_diffs > 1e-6\n",
        "    if not np.any(valid_diffs): return pd.Series(dtype=np.float64), np.array([])\n",
        "\n",
        "    instant_bpm = 60.0 / time_diffs[valid_diffs]\n",
        "    start_time = datetime.datetime.fromtimestamp(0)\n",
        "    valid_peak_times_dt = [start_time + datetime.timedelta(seconds=t) for t in peak_times[1:][valid_diffs]]\n",
        "    bpm_series = pd.Series(instant_bpm, index=valid_peak_times_dt)\n",
        "    avg_heart_rate = np.median(instant_bpm)\n",
        "    if avg_heart_rate > 0:\n",
        "        smoothing_window_sec = params['output_smoothing_window_sec']\n",
        "        smoothing_window_str = f\"{smoothing_window_sec}s\"\n",
        "        smoothed_bpm = bpm_series.rolling(window=smoothing_window_str, min_periods=1, center=True).mean()\n",
        "    else:\n",
        "        smoothed_bpm = pd.Series(dtype=np.float64)\n",
        "\n",
        "    # Return the original numpy time points for compatibility with older functions that need it\n",
        "    return smoothed_bpm, peak_times[1:][valid_diffs]\n",
        "\n",
        "def find_major_hr_inclines(smoothed_bpm_series: pd.Series, min_duration_sec: int = 10, min_bpm_increase: int = 15) -> List[Dict]:\n",
        "    \"\"\"Identifies significant, sustained periods of heart rate increase.\"\"\"\n",
        "    if smoothed_bpm_series.empty or len( smoothed_bpm_series) < 2:\n",
        "        return []\n",
        "\n",
        "    logging.info(f\"Searching for major HR inclines (min_duration={min_duration_sec}s, min_increase={min_bpm_increase} BPM)...\")\n",
        "    time_diffs_sec = smoothed_bpm_series.index.to_series().diff().dt.total_seconds()\n",
        "    mean_time_diff = np.nanmean(time_diffs_sec)\n",
        "    distance_samples = 5 if np.isnan(mean_time_diff) or mean_time_diff == 0 else int((min_duration_sec / 2) / mean_time_diff)\n",
        "\n",
        "    peaks, _ = find_peaks(smoothed_bpm_series.values, prominence=5, distance=distance_samples)\n",
        "    troughs, _ = find_peaks(-smoothed_bpm_series.values, prominence=5, distance=distance_samples)\n",
        "    logging.info(f\"Found {len(troughs)} potential start points (troughs) and {len(peaks)} potential end points (peaks) for inclines.\")\n",
        "    if len(troughs) == 0 or len(peaks) == 0:\n",
        "        return []\n",
        "\n",
        "    major_inclines = []\n",
        "    for trough_idx in troughs:\n",
        "        following_peaks_indices = peaks[peaks > trough_idx]\n",
        "        if len(following_peaks_indices) > 0:\n",
        "            peak_idx = following_peaks_indices[0]\n",
        "            start_time, end_time = smoothed_bpm_series.index[trough_idx], smoothed_bpm_series.index[peak_idx]\n",
        "            start_bpm, end_bpm = smoothed_bpm_series.values[trough_idx], smoothed_bpm_series.values[peak_idx]\n",
        "            duration, bpm_increase = (end_time - start_time).total_seconds(), end_bpm - start_bpm\n",
        "\n",
        "            if duration >= min_duration_sec and bpm_increase >= min_bpm_increase:\n",
        "                major_inclines.append({\n",
        "                    'start_time': start_time, 'end_time': end_time, 'start_bpm': start_bpm, 'end_bpm': end_bpm,\n",
        "                    'duration_sec': duration, 'bpm_increase': bpm_increase, 'slope_bpm_per_sec': bpm_increase / duration\n",
        "                })\n",
        "    major_inclines.sort(key=lambda x: x['slope_bpm_per_sec'], reverse=True)\n",
        "    return major_inclines\n",
        "\n",
        "def find_major_hr_declines(smoothed_bpm_series: pd.Series, min_duration_sec: int = 10, min_bpm_decrease: int = 15) -> List[Dict]:\n",
        "    \"\"\"Identifies significant, sustained periods of heart rate decrease (recovery).\"\"\"\n",
        "    if smoothed_bpm_series.empty or len(smoothed_bpm_series) < 2:\n",
        "        return []\n",
        "\n",
        "    logging.info(f\"Searching for major HR declines (min_duration={min_duration_sec}s, min_decrease={min_bpm_decrease} BPM)...\")\n",
        "    time_diffs_sec = smoothed_bpm_series.index.to_series().diff().dt.total_seconds()\n",
        "    mean_time_diff = np.nanmean(time_diffs_sec)\n",
        "    distance_samples = 5 if np.isnan(mean_time_diff) or mean_time_diff == 0 else int((min_duration_sec / 2) / mean_time_diff)\n",
        "\n",
        "    peaks, _ = find_peaks(smoothed_bpm_series.values, prominence=5, distance=distance_samples)\n",
        "    troughs, _ = find_peaks(-smoothed_bpm_series.values, prominence=5, distance=distance_samples)\n",
        "    logging.info(f\"Found {len(peaks)} potential start points (peaks) and {len(troughs)} potential end points (troughs) for declines.\")\n",
        "    if len(troughs) == 0 or len(peaks) == 0:\n",
        "        return []\n",
        "\n",
        "    major_declines = []\n",
        "    for peak_idx in peaks:\n",
        "        following_troughs_indices = troughs[troughs > peak_idx]\n",
        "        if len(following_troughs_indices) > 0:\n",
        "            trough_idx = following_troughs_indices[0]\n",
        "            start_time, end_time = smoothed_bpm_series.index[peak_idx], smoothed_bpm_series.index[trough_idx]\n",
        "            start_bpm, end_bpm = smoothed_bpm_series.values[peak_idx], smoothed_bpm_series.values[trough_idx]\n",
        "            duration, bpm_decrease = (end_time - start_time).total_seconds(), start_bpm - end_bpm\n",
        "\n",
        "            if duration >= min_duration_sec and bpm_decrease >= min_bpm_decrease:\n",
        "                major_declines.append({\n",
        "                    'start_time': start_time, 'end_time': end_time, 'start_bpm': start_bpm, 'end_bpm': end_bpm,\n",
        "                    'duration_sec': duration, 'bpm_decrease': bpm_decrease, 'slope_bpm_per_sec': (end_bpm - start_bpm) / duration\n",
        "                })\n",
        "    major_declines.sort(key=lambda x: x['slope_bpm_per_sec'])\n",
        "    return major_declines\n",
        "\n",
        "def find_peak_recovery_rate(smoothed_bpm_series: pd.Series, window_sec: int = 20) -> Optional[Dict]:\n",
        "    \"\"\"Finds the steepest slope of heart rate decline after the peak BPM.\"\"\"\n",
        "    if smoothed_bpm_series.empty or len(smoothed_bpm_series) < 2: return None\n",
        "    recovery_series = smoothed_bpm_series[smoothed_bpm_series.idxmax():]\n",
        "    if recovery_series.empty: return None\n",
        "\n",
        "    times_sec = (recovery_series.index - recovery_series.index[0]).total_seconds()\n",
        "    if times_sec[-1] < window_sec: return None\n",
        "\n",
        "    bpm_values, steepest_slope, best_period = recovery_series.values, 0, None\n",
        "    for i in range(len(times_sec) - 1):\n",
        "        end_idx_candidates = np.where(times_sec >= times_sec[i] + window_sec)[0]\n",
        "        if len(end_idx_candidates) == 0: break\n",
        "        end_idx = end_idx_candidates[0]\n",
        "        duration = times_sec[end_idx] - times_sec[i]\n",
        "        if duration > 0:\n",
        "            slope = (bpm_values[end_idx] - bpm_values[i]) / duration\n",
        "            if slope < steepest_slope:\n",
        "                steepest_slope = slope\n",
        "                best_period = {'start_time': recovery_series.index[i], 'end_time': recovery_series.index[end_idx],\n",
        "                               'start_bpm': bpm_values[i], 'end_bpm': bpm_values[end_idx],\n",
        "                               'slope_bpm_per_sec': slope, 'duration_sec': duration}\n",
        "    return best_period\n",
        "\n",
        "def find_peak_exertion_rate(smoothed_bpm_series: pd.Series, window_sec: int = 20) -> Optional[Dict]:\n",
        "    \"\"\"Finds the steepest slope of heart rate increase across the entire recording.\"\"\"\n",
        "    if smoothed_bpm_series.empty or len(smoothed_bpm_series) < 2: return None\n",
        "    times_sec = (smoothed_bpm_series.index - smoothed_bpm_series.index[0]).total_seconds()\n",
        "    if times_sec[-1] < window_sec: return None\n",
        "\n",
        "    bpm_values, steepest_slope, best_period = smoothed_bpm_series.values, 0, None\n",
        "    for i in range(len(times_sec) - 1):\n",
        "        end_idx_candidates = np.where(times_sec >= times_sec[i] + window_sec)[0]\n",
        "        if len(end_idx_candidates) == 0: break\n",
        "        end_idx = end_idx_candidates[0]\n",
        "        duration = times_sec[end_idx] - times_sec[i]\n",
        "        if duration > 0:\n",
        "            slope = (bpm_values[end_idx] - bpm_values[i]) / duration\n",
        "            if slope > steepest_slope:\n",
        "                steepest_slope = slope\n",
        "                best_period = {'start_time': smoothed_bpm_series.index[i], 'end_time': smoothed_bpm_series.index[end_idx],\n",
        "                               'start_bpm': bpm_values[i], 'end_bpm': bpm_values[end_idx],\n",
        "                               'slope_bpm_per_sec': slope, 'duration_sec': duration}\n",
        "    return best_period\n",
        "\n",
        "def calculate_hrr(smoothed_bpm_series: pd.Series, interval_sec: int = 60) -> Optional[Dict]:\n",
        "    \"\"\"Calculates the standard Heart Rate Recovery (HRR) over a fixed interval.\"\"\"\n",
        "    if smoothed_bpm_series.empty or len(smoothed_bpm_series) < 2: return None\n",
        "    peak_bpm, peak_time = smoothed_bpm_series.max(), smoothed_bpm_series.idxmax()\n",
        "    recovery_check_time = peak_time + pd.Timedelta(seconds=interval_sec)\n",
        "    if recovery_check_time > smoothed_bpm_series.index.max(): return None\n",
        "\n",
        "    recovery_bpm = np.interp(\n",
        "        recovery_check_time.timestamp(),\n",
        "        smoothed_bpm_series.index.astype(np.int64) // 10**9,\n",
        "        smoothed_bpm_series.values)\n",
        "    return {'peak_bpm': peak_bpm, 'peak_time': peak_time, 'recovery_bpm': recovery_bpm,\n",
        "            'recovery_check_time': recovery_check_time, 'hrr_value_bpm': peak_bpm - recovery_bpm,\n",
        "            'interval_sec': interval_sec}\n",
        "\n",
        "def find_recovery_phase(bpm_series: pd.Series, bpm_times_sec: np.ndarray, params: Dict) -> Tuple[Optional[float], Optional[float]]:\n",
        "    \"\"\"Analyzes a preliminary BPM series to find the peak heart rate and define the subsequent recovery phase window.\"\"\"\n",
        "    if bpm_times_sec is None or len(bpm_times_sec) < 2:\n",
        "        logging.warning(\"Not enough preliminary beats to determine a recovery phase.\")\n",
        "        return None, None\n",
        "    peak_time_sec = bpm_times_sec[np.argmax(bpm_series.values)]\n",
        "    recovery_end_time_sec = peak_time_sec + params.get(\"recovery_phase_duration_sec\", 120.0)\n",
        "    logging.info(f\"Peak BPM detected in preliminary pass at {peak_time_sec:.2f}s. High-contractility state defined until {recovery_end_time_sec:.2f}s.\")\n",
        "    return peak_time_sec, recovery_end_time_sec\n",
        "\n",
        "# --- Main Analysis Pipeline (Orchestrator) ---\n",
        "def _run_preliminary_pass(audio_envelope: np.ndarray, sample_rate: int, params: Dict,\n",
        "                          noise_floor: pd.Series, troughs: np.ndarray,\n",
        "                          start_bpm_hint: Optional[float]) -> Tuple[float, Optional[float], Optional[float]]:\n",
        "    \"\"\"\n",
        "    Runs a high-confidence first pass to estimate global BPM and find the recovery phase.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- STAGE 2: Running High-Confidence pass to find anchor beats ---\")\n",
        "    params_pass_1 = params.copy()\n",
        "    # Use a higher threshold for a more confident initial beat detection\n",
        "    params_pass_1[\"pairing_confidence_threshold\"] = 0.75\n",
        "\n",
        "    # Use the classifier for a high-confidence dry run\n",
        "    classifier = PeakClassifier(audio_envelope, sample_rate, params_pass_1, start_bpm_hint,\n",
        "                                noise_floor, troughs, None, None)\n",
        "    anchor_beats, _, _ = classifier.classify_peaks()\n",
        "\n",
        "    global_bpm_estimate = None\n",
        "    if len(anchor_beats) >= 10:\n",
        "        median_rr_sec = np.median(np.diff(anchor_beats) / sample_rate)\n",
        "        if median_rr_sec > 0:\n",
        "            global_bpm_estimate = 60.0 / median_rr_sec\n",
        "            logging.info(f\"Automatically determined Global BPM Estimate: {global_bpm_estimate:.1f} BPM\")\n",
        "\n",
        "    # Determine the starting BPM for the main analysis\n",
        "    start_bpm = start_bpm_hint or global_bpm_estimate or 80.0\n",
        "\n",
        "    prelim_bpm_series, prelim_bpm_times = calculate_bpm_series(anchor_beats, sample_rate, params)\n",
        "    peak_bpm_time_sec, recovery_end_time_sec = find_recovery_phase(prelim_bpm_series, prelim_bpm_times, params)\n",
        "\n",
        "    return start_bpm, peak_bpm_time_sec, recovery_end_time_sec\n",
        "\n",
        "\n",
        "def _refine_and_correct_peaks(s1_peaks: np.ndarray, all_raw_peaks: np.ndarray,\n",
        "                              analysis_data: Dict, audio_envelope: np.ndarray,\n",
        "                              sample_rate: int, params: Dict) -> Tuple[np.ndarray, Dict]:\n",
        "    \"\"\"\n",
        "    Applies rhythmic and iterative contextual correction passes to refine S1 peaks.\n",
        "    \"\"\"\n",
        "    logging.info(\"--- STAGES 4 & 5: Refining peaks with rhythmic and contextual correction ---\")\n",
        "\n",
        "    # STAGE 4: Simple rhythmic correction (e.g., remove beats that are too close)\n",
        "    s1_peaks_rhythm_corrected = correct_peaks_by_rhythm(s1_peaks, audio_envelope, sample_rate, params)\n",
        "\n",
        "    # Prepare data for the iterative pass\n",
        "    dynamic_noise_floor = analysis_data['dynamic_noise_floor_series']\n",
        "    current_debug_info = analysis_data[\"beat_debug_info\"].copy()\n",
        "    final_peaks = s1_peaks_rhythm_corrected\n",
        "\n",
        "    # iterative correction loop\n",
        "    max_iterations = 5  # Safeguard against infinite loops\n",
        "    for i in range(max_iterations):\n",
        "        logging.info(f\"Correction Pass Iteration {i + 1}...\")\n",
        "\n",
        "        new_peaks, new_debug_info, corrections_made = _fix_rhythmic_discontinuities(\n",
        "            s1_peaks=final_peaks,\n",
        "            all_raw_peaks=all_raw_peaks,\n",
        "            debug_info=current_debug_info,\n",
        "            audio_envelope=audio_envelope,\n",
        "            dynamic_noise_floor=dynamic_noise_floor,\n",
        "            params=params,\n",
        "            sample_rate=sample_rate\n",
        "        )\n",
        "\n",
        "        final_peaks = new_peaks # s1_peaks_rhythm_corrected\n",
        "        current_debug_info = new_debug_info\n",
        "\n",
        "        if corrections_made == 0:\n",
        "            logging.info(\"Correction process stabilized. Exiting loop.\")\n",
        "            break\n",
        "        else:\n",
        "            logging.info(f\"Made {corrections_made} corrections in iteration {i + 1}.\")\n",
        "    else:\n",
        "        logging.warning(\"Correction process reached max iterations without stabilizing.\")\n",
        "\n",
        "    analysis_data[\"beat_debug_info\"] = current_debug_info\n",
        "    return final_peaks, analysis_data\n",
        "\n",
        "\n",
        "def _calculate_final_metrics(final_peaks: np.ndarray, sample_rate: int, params: Dict) -> Dict:\n",
        "    \"\"\"Calculates all final BPM, HRV, and slope metrics for reporting.\"\"\"\n",
        "    metrics = {}\n",
        "    metrics['smoothed_bpm'], metrics['bpm_times'] = calculate_bpm_series(final_peaks, sample_rate, params)\n",
        "    metrics['major_inclines'] = find_major_hr_inclines(metrics['smoothed_bpm'])\n",
        "    metrics['major_declines'] = find_major_hr_declines(metrics['smoothed_bpm'])\n",
        "    metrics['hrr_stats'] = calculate_hrr(metrics['smoothed_bpm'])\n",
        "    metrics['peak_recovery_stats'] = find_peak_recovery_rate(metrics['smoothed_bpm'])\n",
        "    metrics['peak_exertion_stats'] = find_peak_exertion_rate(metrics['smoothed_bpm'])\n",
        "    metrics['windowed_hrv_df'] = calculate_windowed_hrv(final_peaks, sample_rate, params)\n",
        "\n",
        "    hrv_summary_stats = {}\n",
        "    if not metrics['smoothed_bpm'].empty:\n",
        "        hrv_summary_stats['avg_bpm'] = metrics['smoothed_bpm'].mean()\n",
        "        hrv_summary_stats['min_bpm'] = metrics['smoothed_bpm'].min()\n",
        "        hrv_summary_stats['max_bpm'] = metrics['smoothed_bpm'].max()\n",
        "    if not metrics['windowed_hrv_df'].empty:\n",
        "        hrv_summary_stats['avg_rmssdc'] = metrics['windowed_hrv_df']['rmssdc'].mean()\n",
        "        hrv_summary_stats['avg_sdnn'] = metrics['windowed_hrv_df']['sdnn'].mean()\n",
        "    metrics['hrv_summary'] = hrv_summary_stats\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def analyze_wav_file(wav_file_path: str, params: Dict, start_bpm_hint: Optional[float], original_file_path: str, output_directory: str):\n",
        "    \"\"\"Main analysis pipeline that orchestrates the refactored classes.\"\"\"\n",
        "    start_time = time.time()\n",
        "    logging.info(f\"--- Processing file: {os.path.basename(original_file_path)} ---\")\n",
        "\n",
        "    # STAGE 1: Initialization\n",
        "    audio_envelope, sample_rate = preprocess_audio(wav_file_path, params, output_directory)\n",
        "    noise_floor, troughs = _calculate_dynamic_noise_floor(audio_envelope, sample_rate, params)\n",
        "\n",
        "    start_bpm, peak_time, recovery_time = _run_preliminary_pass(\n",
        "        audio_envelope, sample_rate, params, noise_floor, troughs, start_bpm_hint\n",
        "    )\n",
        "\n",
        "    # STAGE 3: Main Analysis, now informed by the preliminary pass\n",
        "    logging.info(\"--- STAGE 3: Running Main Analysis Pass ---\")\n",
        "    classifier = PeakClassifier(\n",
        "        audio_envelope, sample_rate, params, start_bpm,\n",
        "        noise_floor, troughs, peak_time, recovery_time\n",
        "    )\n",
        "    s1_peaks, all_raw_peaks, analysis_data = classifier.classify_peaks()\n",
        "\n",
        "    # STAGE 4 & 5: Correction and Refinement\n",
        "    final_peaks, analysis_data = _refine_and_correct_peaks(\n",
        "        s1_peaks, all_raw_peaks, analysis_data, audio_envelope, sample_rate, params\n",
        "    )\n",
        "\n",
        "    # STAGE 6: Final Reporting\n",
        "    if len(final_peaks) < 2:\n",
        "        logging.warning(\"Not enough S1 peaks detected to generate full report.\")\n",
        "        return\n",
        "\n",
        "    logging.info(\"--- STAGE 6: Calculating Metrics and Generating Outputs ---\")\n",
        "    final_metrics = _calculate_final_metrics(final_peaks, sample_rate, params)\n",
        "\n",
        "    plotter = Plotter(original_file_path, params, sample_rate, output_directory)\n",
        "    plotter.plot_and_save(audio_envelope, all_raw_peaks, analysis_data, final_metrics)\n",
        "\n",
        "    reporter = ReportGenerator(original_file_path, output_directory)\n",
        "    reporter.save_analysis_summary(final_metrics)\n",
        "    reporter.create_chronological_log(audio_envelope, sample_rate, all_raw_peaks, analysis_data, final_metrics)\n",
        "    reporter.save_analysis_settings(start_bpm_hint)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    logging.info(f\"--- Analysis finished in {duration:.2f} seconds. ---\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Runs BPM analysis on sample_input.wav without GUI.\n",
        "    This is a command-line version of the BPM analyzer.\n",
        "    \"\"\"\n",
        "    # Configure logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - [%(levelname)s] - %(message)s',\n",
        "        stream=sys.stdout\n",
        "    )\n",
        "\n",
        "    # FILE PATH GOES HERE - Edit this path to point to your sample_input.wav file\n",
        "    sample_file_path = os.path.join(\"samples\", \"sample_input.wav\")\n",
        "\n",
        "    # Check if the sample file exists\n",
        "    if not os.path.exists(sample_file_path):\n",
        "        logging.error(f\"Sample file not found at: {sample_file_path}\")\n",
        "        logging.error(\"Please ensure the file exists or update the path in main2.py\")\n",
        "        return\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = os.path.join(os.getcwd(), \"processed_files\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Analysis parameters (using defaults from config)\n",
        "    params = DEFAULT_PARAMS.copy()\n",
        "\n",
        "    # Optional: Set a starting BPM hint (set to None to auto-detect)\n",
        "    start_bpm_hint = None  # You can set this to a specific value like 80.0 if needed\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Starting analysis of: {sample_file_path}\")\n",
        "\n",
        "        # Determine the WAV file path for processing\n",
        "        base_name, ext = os.path.splitext(sample_file_path)\n",
        "        wav_path = os.path.join(output_dir, f\"{os.path.basename(base_name)}.wav\")\n",
        "\n",
        "        # Convert to WAV if needed\n",
        "        if ext.lower() != '.wav':\n",
        "            logging.info(f\"Converting {os.path.basename(sample_file_path)} to WAV format...\")\n",
        "            if not convert_to_wav(sample_file_path, wav_path):\n",
        "                raise Exception(\"File conversion failed.\")\n",
        "        else:\n",
        "            # Copy WAV file to output directory\n",
        "            shutil.copy(sample_file_path, wav_path)\n",
        "\n",
        "        logging.info(\"Starting heartbeat analysis...\")\n",
        "\n",
        "        # Run the analysis\n",
        "        analyze_wav_file(\n",
        "            wav_path,\n",
        "            params,\n",
        "            start_bpm_hint,\n",
        "            original_file_path=sample_file_path,\n",
        "            output_directory=output_dir\n",
        "        )\n",
        "\n",
        "        logging.info(\"Analysis completed successfully!\")\n",
        "        logging.info(f\"Results saved to: {output_dir}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Analysis failed: {str(e)}\")\n",
        "        return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2e_KQy_wT27c",
        "outputId": "91f9742a-3c55-4a5e-a185-7edc30909c68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Original 'downsample_factor' of 300 is too high for a 150Hz filter with a 44100Hz sample rate.\n",
            "WARNING:root:Adjusting 'downsample_factor' to a safe value of 146.\n"
          ]
        }
      ]
    }
  ]
}